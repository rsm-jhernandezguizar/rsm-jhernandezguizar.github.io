[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Analytics Portfolio",
    "section": "",
    "text": "Clustering & Classification Methods\n\n\n \n\nK-means clustering and machine learning applications\n\n\n\nJun 11, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStreaming Service Consumer Preferences\n\n\n \n\nConjoint analysis and multinomial logit modeling\n\n\n\nMay 28, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatent Success & Airbnb Demand Analysis\n\n\n \n\nPoisson and negative binomial regression case studies\n\n\n\nMay 7, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundraising Campaign Effectiveness Analysis\n\n\n \n\nReplication study of matching grant strategies (Karlan & List, 2007)\n\n\n\nApr 23, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/streaming-preferences/Assignment_3.html",
    "href": "portfolio/streaming-preferences/Assignment_3.html",
    "title": "Streaming Service Consumer Preferences",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "portfolio/streaming-preferences/Assignment_3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "portfolio/streaming-preferences/Assignment_3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Streaming Service Consumer Preferences",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "portfolio/streaming-preferences/Assignment_3.html#simulate-conjoint-data",
    "href": "portfolio/streaming-preferences/Assignment_3.html#simulate-conjoint-data",
    "title": "Streaming Service Consumer Preferences",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nTo visualize this we developed a simulation of the conjoin data below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1\n\n\n\n\n\n\n\n\n\nThe simulated dataset contains 3000 rows and 6 columns (exactly 100 respondents × 10 tasks × 3 alternatives). Just as expected!"
  },
  {
    "objectID": "portfolio/streaming-preferences/Assignment_3.html#preparing-the-data-for-estimation",
    "href": "portfolio/streaming-preferences/Assignment_3.html#preparing-the-data-for-estimation",
    "title": "Streaming Service Consumer Preferences",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nBelow, we load the conjoint dataset and reshape it for estimation. We create dummy variables for the brand and ad features (using Hulu and “No Ads” as the base levels), and we construct a task identifier to group alternatives belonging to the same choice set. We then display the first few rows to verify the structure:\n\n\n   resp  task  choice brand   ad  price  brand_N  brand_P  ad_yes  task_id\n0     1     1       1     N  Yes     28        1        0       1        0\n1     1     1       0     H  Yes     16        0        0       1        0\n2     1     1       0     P  Yes     16        0        1       1        0\n3     1     2       0     N  Yes     32        1        0       1        1\n4     1     2       1     P  Yes     16        0        1       1        1\n\n\nWe can see that each choice task (task_id) contains three rows (one per alternative). For example, resp=1, task=1 (first three rows) had alternatives from brands N, H, P all with ads (“Yes”) and different prices, and the first row (brand=N, price=28) was marked choice=1 as the selected option (it had the highest simulated utility). The dataset is now ready for model estimation."
  },
  {
    "objectID": "portfolio/streaming-preferences/Assignment_3.html#estimation-via-maximum-likelihood",
    "href": "portfolio/streaming-preferences/Assignment_3.html#estimation-via-maximum-likelihood",
    "title": "Streaming Service Consumer Preferences",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWith the data prepared, we now turn to estimating the MNL model parameters by Maximum Likelihood. We have four parameters to estimate: \\(\\beta_{\\text{Netflix}}\\) and \\(\\beta_{\\text{Prime}}\\) for the two non-baseline brands (Hulu is the baseline, so its effect is 0 by construction), \\(\\beta_{\\text{Ads}}\\) for the effect of having ads (versus no ads), and \\(\\beta_{\\text{Price}}\\) for the price coefficient.\nLog-Likelihood Function: First, we need to code up the log-likelihood function for the MNL. Using the data, the log-likelihood \\(\\ell_n(\\beta)\\) can be computed by summing, over all choice tasks, the log of the probability of the chosen alternative. Given our data structure, a convenient method is: for each choice task, find the linear utility \\(v_{ij} = x_j’ \\beta\\) for each alternative, compute the choice probabilities \\(\\mathbb{P}_i(j)\\) via the softmax formula, then accumulate \\(\\log \\mathbb{P}_i(j^)\\) for the chosen alternative \\(j^{*}\\). We implement this below. To speed up computation, we vectorize the operations using NumPy:\n\nimport numpy as np\n\n# Extract numpy arrays for faster computation\nX = df[[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]].values  # Feature matrix (3000 x 4)\ngroup_ids = df[\"task_id\"].values  # Task identifiers (length 3000)\nchoice = df[\"choice\"].values  # Chosen indicator (0/1 for each row)\n\n\n# Define the log-likelihood function for parameters beta (as a NumPy array)\ndef loglik(beta):\n    beta = np.array(beta)\n    # Compute linear utility v = X * beta for all alternatives\n    v = X.dot(beta)  # shape (3000,)\n    # Compute exp(v) and sum exp(v) by task (denominator of softmax)\n    exp_v = np.exp(v)\n    # Sum of exp(v) for each task (using group_ids to aggregate)\n    sum_exp_v_by_task = np.bincount(\n        group_ids, weights=exp_v, minlength=df[\"task_id\"].nunique()\n    )\n    # Sum of v for the chosen alternative in each task (only one chosen per task)\n    chosen_v_by_task = np.bincount(\n        group_ids, weights=v * choice, minlength=df[\"task_id\"].nunique()\n    )\n    # Log-likelihood is sum over tasks of (v_chosen - log(sum_exp_v))\n    log_lik_value = np.sum(chosen_v_by_task - np.log(sum_exp_v_by_task))\n    return log_lik_value\n\n\n# Quick sanity check: compute log-likelihood at the true beta values used in simulation\nbeta_true = np.array([1.0, 0.5, -0.8, -0.1])\nprint(f\"Log-likelihood at true beta: {loglik(beta_true):.3f}\")\n\nLog-likelihood at true beta: -880.344\n\n\nWe included a quick check: plugging in the true part-worths [1.0, 0.5, -0.8, -0.1] gives a log-likelihood of roughly -880.3. Now, we will let the computer search for the \\(\\beta\\) that maximizes the log-likelihood. In practice, we maximize the likelihood by minimizing the negative log-likelihood. We can use a numerical optimizer (from SciPy in Python) to find the Maximum Likelihood Estimates (MLEs) of \\(\\beta\\). We also compute the Hessian-based standard errors and 95% confidence intervals for these estimates. The table below summarizes the results:\n\n\nParameter  Estimate  Std. Error 95% Confidence Interval\nβ_Netflix     0.941       0.114          [0.717, 1.165]\n  β_Prime     0.502       0.121          [0.265, 0.738]\n    β_Ads    -0.732       0.089        [-0.906, -0.558]\n  β_Price    -0.099       0.006        [-0.112, -0.087]\n\n\nAll four estimates are very close to the true values used in the simulation, which is reassuring. The estimate for β_Netflix is about 0.94 (true was 1.0) and for β_Prime about 0.50 (true 0.5), with Hulu as the baseline (so Hulu’s implicit β is 0). This means that, holding ads and price constant, a Netflix offering has about 0.94 higher utility units than an otherwise identical Hulu offering, and Prime has 0.50 higher utility than Hulu. The Ads coefficient is -0.732, indicating a strong negative effect of having advertisements: an offering with ads is less attractive by ~0.73 utility units compared to an ad-free equivalent. The Price coefficient is -0.099, meaning each additional $1 per month reduces utility by ~0.099. All parameters are significantly different from zero at the 95% confidence level (zero lies outside all the confidence intervals), aligning with our expectations (e.g., higher price and ads included both significantly reduce the likelihood of choice)."
  },
  {
    "objectID": "portfolio/streaming-preferences/Assignment_3.html#estimation-via-bayesian-methods",
    "href": "portfolio/streaming-preferences/Assignment_3.html#estimation-via-bayesian-methods",
    "title": "Streaming Service Consumer Preferences",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nNext, we estimate the MNL model via a Bayesian approach. Rather than finding a single best estimate as in MLE, Bayesian inference will produce a posterior distribution for the parameters. We use a Metropolis-Hastings (M-H) algorithm (a type of Markov Chain Monte Carlo) to sample from the posterior distribution of \\(\\beta\\).\nPriors: We choose relatively non-informative (weak) priors for the parameters. For the binary feature coefficients (\\(\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}}\\)) we use independent priors \\(\\mathcal{N}(0,;5)\\) – a normal distribution with mean 0 and variance 5 (standard deviation \\(\\approx 2.236\\)). For the price coefficient \\(\\beta_{\\text{Price}}\\), we use a slightly more informative prior \\(\\mathcal{N}(0,;1)\\) (mean 0, variance 1) since price effects are often tighter in range. These priors still cover a wide range of plausible values relative to our expected magnitudes, essentially adding only a gentle regularization around 0.\nMCMC Setup: We will run the M-H sampler for 11,000 iterations and discard the first 1,000 draws as “burn-in” to allow the chain to converge. This will leave us with 10,000 posterior draws for inference. We construct a symmetric proposal distribution as a multivariate normal with no covariance between parameters (diagonal covariance matrix). In particular, we use independent normal proposal steps with standard deviations: 0.05 for each of \\(\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}}\\), and 0.005 for \\(\\beta_{\\text{Price}}\\). These step sizes (chosen based on the scale of the data and priors) should yield a reasonable acceptance rate for the M-H algorithm.\nBelow is the Python implementation of the Metropolis-Hastings sampler for the posterior of \\(\\beta\\):\n\nimport math\n\n\n# Define log-posterior function (log-likelihood + log-prior)\ndef log_posterior(beta):\n    # log-likelihood from data:\n    log_lik_val = loglik(beta)\n    # log-prior for each parameter:\n    # Prior variances: 5 for Netflix/Prime/Ads, 1 for Price\n    beta = np.array(beta)\n    # Log-prior for three binary attribute betas ~ N(0,5)\n    var_bin = 5.0\n    log_prior_bin = -0.5 * (\n        3 * math.log(2 * math.pi * var_bin) + np.sum(beta[:3] ** 2) / var_bin\n    )\n    # Log-prior for price beta ~ N(0,1)\n    var_price = 1.0\n    log_prior_price = -0.5 * (\n        math.log(2 * math.pi * var_price) + (beta[3] ** 2) / var_price\n    )\n    return log_lik_val + log_prior_bin + log_prior_price\n\n\n# M-H sampling parameters\niterations = 11000\nburn_in = 1000\n\n# Proposal distribution standard deviations for [Netflix, Prime, Ads, Price]\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\n# Initialize chain and starting value (use MLE as starting point for efficiency)\nchain = np.zeros((iterations, 4))\nchain[0] = beta_hat  # start at MLE estimate (could also start at [0,0,0,0])\n\ncurrent_beta = chain[0].copy()\ncurrent_logpost = log_posterior(current_beta)\n\naccept_count = 0\nfor t in range(1, iterations):\n    # Propose a new beta by adding random normal noise to each dimension\n    proposal = current_beta + np.random.normal(0, proposal_sd, size=4)\n    prop_logpost = log_posterior(proposal)\n    # Metropolis-Hastings acceptance probability\n    log_accept_ratio = prop_logpost - current_logpost\n    if math.log(np.random.rand()) &lt; log_accept_ratio:\n        # Accept proposal\n        current_beta = proposal\n        current_logpost = prop_logpost\n        chain[t] = proposal\n        accept_count += 1\n    else:\n        # Reject proposal (retain current beta)\n        chain[t] = current_beta\n\naccept_rate = accept_count / (iterations - 1)\nprint(f\"Acceptance rate: {accept_rate:.3f}\")\n\nAcceptance rate: 0.566\n\n\nWe print the acceptance rate to ensure the sampler is moving adequately. In this run, the acceptance rate is around 50-60%, indicating the proposal step sizes are reasonable for efficient mixing (neither too high nor too low).\nAfter discarding the first 1,000 draws as burn-in, we use the remaining 10,000 posterior draws to analyze the posterior distribution of each parameter. To illustrate convergence and the posterior distribution, the following figures show the trace and the histogram of the posterior sample for one of the parameters (here we choose \\(\\beta_{\\text{Netflix}}\\) as an example):\n\n\n\n\n\n\n\n\n\nTrace plot of the MCMC chain for \\(\\beta_{\\text{Netflix}}\\) after burn-in. The chain fluctuates around its mean (red dashed line), indicating good mixing.\n\n\n\n\n\n\n\n\n\nHistogram of the posterior draws for \\(\\beta_{\\text{Netflix}}\\). The distribution is approximately normal. The red line marks the posterior mean, and the black dotted lines mark the 95% credible interval bounds.\nUsing the posterior sample, we can summarize the Bayesian estimates for all four parameters. Below we report the posterior mean, standard deviation, and 95% credible interval for each \\(\\beta\\), and compare them to the earlier MLE results: • \\(\\beta_{\\text{Netflix}}\\): Posterior mean = 0.941, SD = 0.112, 95% credible interval [0.721, 1.157]. • \\(\\beta_{\\text{Prime}}\\): Posterior mean = 0.502, SD = 0.113, 95% credible interval [0.277, 0.723]. • \\(\\beta_{\\text{Ads}}\\): Posterior mean = -0.734, SD = 0.088, 95% credible interval [-0.907, -0.566]. • \\(\\beta_{\\text{Price}}\\): Posterior mean = -0.100, SD = 0.006, 95% credible interval [-0.112, -0.088].\nThese posterior estimates are virtually identical to the MLE results we obtained earlier. The weak priors did not substantially pull the estimates toward zero, so the posterior means (0.941, 0.502, -0.734, -0.100) are almost the same as the MLE point estimates (0.941, 0.502, -0.732, -0.099). Likewise, the posterior standard deviations are in line with the MLE standard errors, and the 95% credible intervals correspond closely to the 95% confidence intervals from the likelihood approach. This consistency provides a nice validation: given our large sample (1000 choice tasks) and relatively uninformative priors, the Bayesian and frequentist approaches yield the same substantive conclusions."
  },
  {
    "objectID": "portfolio/streaming-preferences/Assignment_3.html#discussion",
    "href": "portfolio/streaming-preferences/Assignment_3.html#discussion",
    "title": "Streaming Service Consumer Preferences",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we consider these results as if they came from a real conjoint study (i.e. not knowing the “true” simulation values), we can interpret the parameter estimates in practical terms. We observe that \\(\\beta_{\\text{Netflix}}\\) (approximately 0.94) is larger than \\(\\beta_{\\text{Prime}}\\) (about 0.50). This indicates that, on average, consumers in the study derive more utility from the Netflix brand than from Amazon Prime, with Hulu as the baseline (zero effect). In other words, Netflix is the most preferred streaming brand among the three, and Amazon Prime is also preferred to Hulu but less so than Netflix. The negative \\(\\beta_{\\text{Ads}} \\approx -0.73\\) confirms that including advertisements substantially lowers consumer utility compared to an ad-free experience. Similarly, \\(\\beta_{\\text{Price}}\\) is about -0.10, meaning higher price has a negative effect on choice probability (which makes intuitive sense – consumers prefer cheaper options, all else equal).\n\nIt is important to note that \\(\\beta_{\\text{Price}}\\) being negative is not only sensible, but we can also quantify its meaning: a one-unit increase in utility is equivalent to about $10 in price (since \\(1/0.10 = 10\\)). Therefore, the brand coefficient for Netflix (0.94) can be interpreted as Netflix providing roughly $9–$10 worth of added value to consumers relative to Hulu, and Prime’s brand value is about $5 relative to Hulu. Likewise, having ads (\\(\\beta_{\\text{Ads}} \\approx -0.73\\)) imposes a disutility roughly equivalent to $7–$8 per month in price. These insights are very useful for business decisions – for instance, they indicate how much more a company could potentially charge for a Netflix-branded service (or how much discount would be required to compensate for including ads).\nFinally, how would we extend this to a multi-level (random-parameter) model? In a multi-level or hierarchical Bayesian conjoint model, we acknowledge that different consumers may have different preference parameters. Instead of one \\(\\beta\\) vector for the whole population, we assume each individual \\(i\\) has their own \\(\\beta_i\\), drawn from a population distribution (for example, \\(\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\)). To simulate data from such a model, we would first draw each respondent’s true part-worths from a specified distribution (with some mean and variance to represent preference heterogeneity), and then use those individual-level \\(\\beta_i\\) to simulate each person’s choices. To estimate a hierarchical model, we would need to introduce additional layers in our estimation procedure – essentially estimating both the individual-level \\(\\beta_i\\) for each respondent and the hyperparameters (like the mean vector \\(\\mu\\) and covariance \\(\\Sigma\\) of the population distribution). This could be done via hierarchical Bayesian methods (where we would add Gibbs sampling or more complex MCMC steps to sample each \\(\\beta_i\\) and the hyperparameters) or via simulated maximum likelihood (also known as a mixed logit approach). In essence, the key change is moving from a fixed-effects MNL (one common \\(\\beta\\) for all) to a random-effects MNL where each consumer has their own \\(\\beta\\) drawn from a higher-level distribution. This addition would capture the real-world preference heterogeneity we expect in conjoint analysis, at the cost of a more complex simulation and estimation process.\nIn this blog, we successfully implemented and compared maximum likelihood and Bayesian estimation for a multinomial logit conjoint model. Both approaches recovered the true part-worth parameters closely, reinforcing our understanding of MNL. From a business perspective, the results highlight clear patterns: brand matters (Netflix holds a strong advantage over competitors), ads are detrimental to user utility, and price sensitivity is significant. These findings imply that a streaming service can command a premium for a stronger brand or no-ad experience, whereas introducing ads or raising prices must be balanced against the substantial utility loss. By quantifying these trade-offs, conjoint analysis provides valuable guidance for product design and pricing strategy in the streaming industry."
  },
  {
    "objectID": "portfolio/clustering-classification/Assignment_4.html",
    "href": "portfolio/clustering-classification/Assignment_4.html",
    "title": "Clustering & Classification Methods",
    "section": "",
    "text": "K-Means is an unsupervised learning algorithm that groups unlabeled data into k clusters based on similarity. The goal is to partition the data so that points in the same cluster are more similar to each other than to those in other clusters ￼. In essence, K-Means tries to find cluster centers (called centroids) that minimize the distance of each point to its nearest centroid ￼.\nHow does K-Means work? At a high level, the algorithm follows an iterative refinement procedure:\n\nInitialize – Choose k initial centroids (often random picks from the data).\nAssign – For each point, find the nearest centroid (by Euclidean distance) and assign the point to that cluster.\nUpdate – Recompute each centroid as the average (mean) of all points assigned to it.\nRepeat – Iterate the assign-update steps until centroids stop changing (convergence).\n\nThis process will partition the dataset into k clusters such that each point belongs to the cluster with the closest centroid. The algorithm stops when successive iterations no longer change the centroids (or change them negligibly), meaning the clustering has stabilized. The result is a set of clusters and their centroid locations.\nTo demonstrate K-Means, we’ll use the Palmer Penguins dataset, a popular alternative to the iris dataset. It contains measurements for three penguin species (Adelie, Chinstrap, Gentoo) from islands in Antarctica. We will use just two features for clustering: bill length and flipper length (both in mm). This gives us a 2D dataset that we can easily visualize. We will ignore the species labels during clustering (since K-Means is unsupervised), but it’s worth noting there are 3 true species in the data (which might correspond to 3 clusters).\nFirst, let’s load the dataset and take a peek at the data structure:\n\n\n  species  bill_length_mm  flipper_length_mm\n0  Adelie            39.1                181\n1  Adelie            39.5                186\n2  Adelie            40.3                195\n3  Adelie            36.7                193\n4  Adelie            39.3                190\nDataset shape: (333, 2)\n\n\nWe have 332 penguin observations with bill length and flipper length. Now, let’s implement the K-Means algorithm from scratch for a chosen number of clusters, K=3. (Choosing 3 is a reasonable guess here given the three species, but we will later analyze different k values.)\n\n\nWe’ll write a simple implementation of K-Means. The plan:\n\nRandomly initialize 3 centroids by selecting 3 random points from the dataset.\nLoop until convergence:\n\n\nCompute the distance from each data point to each centroid.\nAssign each point to the nearest centroid (forming 3 clusters).\nRecompute each centroid as the mean of the points in its cluster.\nIf centroids don’t change (or change very little), break out.\n\nWe’ll also keep track of the cluster assignments at each iteration so we can visualize the progression.\n\nimport numpy as np\n\n# Prepare data as a numpy array for convenience\nX = penguins.to_numpy()\n\n# K-Means parameters\nK = 3\nnp.random.seed(42)\n# Randomly choose K unique indices for initial centroids\ninitial_idx = np.random.choice(len(X), K, replace=False)\ncentroids = X[initial_idx]\nprint(\"Initial centroids (randomly chosen):\\n\", centroids)\n\n# K-Means iterative process\nmax_iters = 100\ncentroid_history = [centroids.copy()]  # store centroids at each iteration\ncluster_history = []  # store cluster assignments at each iteration\n\nfor itr in range(max_iters):\n    # Step 1: Assign points to the nearest centroid\n    distances = np.linalg.norm(\n        X[:, None] - centroids[None, :], axis=2\n    )  # distance to each centroid\n    clusters = np.argmin(distances, axis=1)  # index of nearest centroid for each point\n    cluster_history.append(clusters)\n\n    # Step 2: Update centroids to the mean of assigned points\n    new_centroids = np.array(\n        [\n            X[clusters == k].mean(axis=0) if np.any(clusters == k) else centroids[k]\n            for k in range(K)\n        ]\n    )\n    # Check for convergence (if centroids are unchanged)\n    if np.allclose(new_centroids, centroids):\n        centroids = new_centroids\n        centroid_history.append(centroids.copy())\n        print(f\"Converged after {itr} iterations.\")\n        break\n    centroids = new_centroids\n    centroid_history.append(centroids.copy())\n\n# Final centroids and cluster assignment\nfinal_centroids = centroids\nfinal_clusters = cluster_history[-1]\nprint(\"Final centroids:\\n\", final_centroids)\n\nInitial centroids (randomly chosen):\n [[ 39.5 178. ]\n [ 50.9 196. ]\n [ 42.1 195. ]]\nConverged after 9 iterations.\nFinal centroids:\n [[ 38.45304348 187.05217391]\n [ 47.6296     216.92      ]\n [ 45.95483871 196.7311828 ]]\n\n\nNext, we’ll visualize the clustering process to see how K-Means reached this result.\n\n\n\nTo better understand K-Means, it helps to visualize how the centroids move and how points switch clusters over iterations. We will plot the data points colored by their cluster at each iteration, and show the centroid positions. An animated GIF can illustrate the process over time. Below, we generate plots for each iteration and combine them into a GIF:\n\nNow we have an animation showing the algorithm’s progress. In the first frame, the centroids start at random positions. With each iteration, points get re-assigned (colors may change), and centroids move towards the center of their new clusters (arrows show the movement). The process continues until the movements are negligible.\nK-Means clustering on the penguins data – after the first iteration. Arrows indicate how the centroids (black X markers) moved from their initial random positions to new positions (black circles) after re-computing the means for each cluster.\nFinal cluster assignment for K=3 on the Palmer Penguins data (bill length vs flipper length). Each color represents a cluster found by our algorithm, and black X’s are the final centroid positions.\nIn the final clustering above, we see three distinct clusters of penguins. It turns out these clusters largely correspond to the three species (Adelie, Chinstrap, Gentoo), even though we did not use the species labels during clustering. Our K-Means algorithm essentially “discovered” groupings similar to the actual species by just using the two features.\n\n\n\nTo ensure our implementation is correct, we can compare with scikit-learn’s built-in K-Means. Scikit-learn uses the same objective (minimizing sum of squared distances) and by default uses the K-Means++ initialization for centroids (a smart way to pick initial centroids). We fit scikit’s model on the same data:\n\n\nSklearn final centroids:\n [[ 45.94574468 196.84042553]\n [ 47.65       217.        ]\n [ 38.45304348 187.05217391]]\n\n\nIf we check, the centroids from scikit-learn are very close to those from our implementation, and the cluster assignments are effectively the same (just label order might differ). This gives us confidence that our scratch implementation worked correctly.\n\n\n\nIn practice, we usually don’t know the best value of K upfront. Choosing K is part of the challenge in clustering. Two common methods to evaluate different K values are:\n\nWithin-Cluster Sum of Squares (WCSS): This is the sum of squared distances from each point to its cluster centroid (also known as cluster “inertia”). Lower WCSS means clusters are tighter. As K increases, WCSS always decreases (more clusters reduce within-cluster variance). We can plot WCSS for K=2,3,… and look for an “elbow point” – where the rate of improvement slows. This is the Elbow Method.\nSilhouette Score: This measures how well-separated the clusters are. Silhouette score for a point is defined as (b - a) / max(a, b), where a is the average distance to other points in the same cluster (cohesion) and b is the average distance to points in the nearest other cluster (separation) ￼. The score ranges from -1 to 1; higher means the point is in the right cluster (well separated from others) ￼. We often use the average silhouette over all points to evaluate the clustering quality for a given K. A higher average silhouette indicates more distinct clustering structure.\n\nLet’s compute these metrics for K = 2 through 7 and see which K might be best for our penguin data:\n\n\nWCSS for K=2..7: [20949.7853112782, 13859.543763777465, 9587.135276652696, 7424.8815080702425, 6332.78650281499, 5257.088985919957]\nSilhouette scores for K=2..7: [0.6117940477662409, 0.48062618684371067, 0.4448839684032104, 0.42719392387268307, 0.41278625329909713, 0.4307326685776095]\n\n\nPlotting these values:\n\n\n\n\n\n\n\n\n\nEvaluating different cluster counts on the penguin data. Left: WCSS (sum of squared distances within clusters) for K=2 to 7. Right: Average silhouette score for K=2 to 7. Higher silhouette is better.\nFrom the above, WCSS steadily decreases as K increases (as expected), and there’s a bit of a knee around K=3-4 (the “elbow” is not super sharp here). The silhouette score peaks at K=2 (about 0.61) and then drops for higher K, hitting a low around 0.41–0.44 for K=4-6, with a slight uptick at K=7. A high silhouette means clusters are well separated, so this suggests that using 2 clusters yields the clearest separation in this 2D feature space.\nThis is interesting because we know there are 3 species. What’s happening is that two of the species (Adelie and Chinstrap) have very similar bill/flipper measurements that overlap a lot, so the algorithm doesn’t find a clear separation between them and favors combining them into one cluster. The Gentoo penguins, on the other hand, form a very distinct cluster. Thus, K=2 gives two well-separated clusters (basically “Gentoo” and “Adelie+Chinstrap”), which yields a higher silhouette score than K=3 where the algorithm is forced to split the Adelie/Chinstrap group and ends up with one weaker separation. In a real analysis, we’d consider this trade-off: do we want 3 clusters to match known species, or 2 clusters that are most distinct in these features? There’s no single correct answer – it depends on the goal. But this illustrates how WCSS and silhouette can guide the choice of K."
  },
  {
    "objectID": "portfolio/clustering-classification/Assignment_4.html#k-means-clustering",
    "href": "portfolio/clustering-classification/Assignment_4.html#k-means-clustering",
    "title": "Clustering & Classification Methods",
    "section": "",
    "text": "K-Means is an unsupervised learning algorithm that groups unlabeled data into k clusters based on similarity. The goal is to partition the data so that points in the same cluster are more similar to each other than to those in other clusters ￼. In essence, K-Means tries to find cluster centers (called centroids) that minimize the distance of each point to its nearest centroid ￼.\nHow does K-Means work? At a high level, the algorithm follows an iterative refinement procedure:\n\nInitialize – Choose k initial centroids (often random picks from the data).\nAssign – For each point, find the nearest centroid (by Euclidean distance) and assign the point to that cluster.\nUpdate – Recompute each centroid as the average (mean) of all points assigned to it.\nRepeat – Iterate the assign-update steps until centroids stop changing (convergence).\n\nThis process will partition the dataset into k clusters such that each point belongs to the cluster with the closest centroid. The algorithm stops when successive iterations no longer change the centroids (or change them negligibly), meaning the clustering has stabilized. The result is a set of clusters and their centroid locations.\nTo demonstrate K-Means, we’ll use the Palmer Penguins dataset, a popular alternative to the iris dataset. It contains measurements for three penguin species (Adelie, Chinstrap, Gentoo) from islands in Antarctica. We will use just two features for clustering: bill length and flipper length (both in mm). This gives us a 2D dataset that we can easily visualize. We will ignore the species labels during clustering (since K-Means is unsupervised), but it’s worth noting there are 3 true species in the data (which might correspond to 3 clusters).\nFirst, let’s load the dataset and take a peek at the data structure:\n\n\n  species  bill_length_mm  flipper_length_mm\n0  Adelie            39.1                181\n1  Adelie            39.5                186\n2  Adelie            40.3                195\n3  Adelie            36.7                193\n4  Adelie            39.3                190\nDataset shape: (333, 2)\n\n\nWe have 332 penguin observations with bill length and flipper length. Now, let’s implement the K-Means algorithm from scratch for a chosen number of clusters, K=3. (Choosing 3 is a reasonable guess here given the three species, but we will later analyze different k values.)\n\n\nWe’ll write a simple implementation of K-Means. The plan:\n\nRandomly initialize 3 centroids by selecting 3 random points from the dataset.\nLoop until convergence:\n\n\nCompute the distance from each data point to each centroid.\nAssign each point to the nearest centroid (forming 3 clusters).\nRecompute each centroid as the mean of the points in its cluster.\nIf centroids don’t change (or change very little), break out.\n\nWe’ll also keep track of the cluster assignments at each iteration so we can visualize the progression.\n\nimport numpy as np\n\n# Prepare data as a numpy array for convenience\nX = penguins.to_numpy()\n\n# K-Means parameters\nK = 3\nnp.random.seed(42)\n# Randomly choose K unique indices for initial centroids\ninitial_idx = np.random.choice(len(X), K, replace=False)\ncentroids = X[initial_idx]\nprint(\"Initial centroids (randomly chosen):\\n\", centroids)\n\n# K-Means iterative process\nmax_iters = 100\ncentroid_history = [centroids.copy()]  # store centroids at each iteration\ncluster_history = []  # store cluster assignments at each iteration\n\nfor itr in range(max_iters):\n    # Step 1: Assign points to the nearest centroid\n    distances = np.linalg.norm(\n        X[:, None] - centroids[None, :], axis=2\n    )  # distance to each centroid\n    clusters = np.argmin(distances, axis=1)  # index of nearest centroid for each point\n    cluster_history.append(clusters)\n\n    # Step 2: Update centroids to the mean of assigned points\n    new_centroids = np.array(\n        [\n            X[clusters == k].mean(axis=0) if np.any(clusters == k) else centroids[k]\n            for k in range(K)\n        ]\n    )\n    # Check for convergence (if centroids are unchanged)\n    if np.allclose(new_centroids, centroids):\n        centroids = new_centroids\n        centroid_history.append(centroids.copy())\n        print(f\"Converged after {itr} iterations.\")\n        break\n    centroids = new_centroids\n    centroid_history.append(centroids.copy())\n\n# Final centroids and cluster assignment\nfinal_centroids = centroids\nfinal_clusters = cluster_history[-1]\nprint(\"Final centroids:\\n\", final_centroids)\n\nInitial centroids (randomly chosen):\n [[ 39.5 178. ]\n [ 50.9 196. ]\n [ 42.1 195. ]]\nConverged after 9 iterations.\nFinal centroids:\n [[ 38.45304348 187.05217391]\n [ 47.6296     216.92      ]\n [ 45.95483871 196.7311828 ]]\n\n\nNext, we’ll visualize the clustering process to see how K-Means reached this result.\n\n\n\nTo better understand K-Means, it helps to visualize how the centroids move and how points switch clusters over iterations. We will plot the data points colored by their cluster at each iteration, and show the centroid positions. An animated GIF can illustrate the process over time. Below, we generate plots for each iteration and combine them into a GIF:\n\nNow we have an animation showing the algorithm’s progress. In the first frame, the centroids start at random positions. With each iteration, points get re-assigned (colors may change), and centroids move towards the center of their new clusters (arrows show the movement). The process continues until the movements are negligible.\nK-Means clustering on the penguins data – after the first iteration. Arrows indicate how the centroids (black X markers) moved from their initial random positions to new positions (black circles) after re-computing the means for each cluster.\nFinal cluster assignment for K=3 on the Palmer Penguins data (bill length vs flipper length). Each color represents a cluster found by our algorithm, and black X’s are the final centroid positions.\nIn the final clustering above, we see three distinct clusters of penguins. It turns out these clusters largely correspond to the three species (Adelie, Chinstrap, Gentoo), even though we did not use the species labels during clustering. Our K-Means algorithm essentially “discovered” groupings similar to the actual species by just using the two features.\n\n\n\nTo ensure our implementation is correct, we can compare with scikit-learn’s built-in K-Means. Scikit-learn uses the same objective (minimizing sum of squared distances) and by default uses the K-Means++ initialization for centroids (a smart way to pick initial centroids). We fit scikit’s model on the same data:\n\n\nSklearn final centroids:\n [[ 45.94574468 196.84042553]\n [ 47.65       217.        ]\n [ 38.45304348 187.05217391]]\n\n\nIf we check, the centroids from scikit-learn are very close to those from our implementation, and the cluster assignments are effectively the same (just label order might differ). This gives us confidence that our scratch implementation worked correctly.\n\n\n\nIn practice, we usually don’t know the best value of K upfront. Choosing K is part of the challenge in clustering. Two common methods to evaluate different K values are:\n\nWithin-Cluster Sum of Squares (WCSS): This is the sum of squared distances from each point to its cluster centroid (also known as cluster “inertia”). Lower WCSS means clusters are tighter. As K increases, WCSS always decreases (more clusters reduce within-cluster variance). We can plot WCSS for K=2,3,… and look for an “elbow point” – where the rate of improvement slows. This is the Elbow Method.\nSilhouette Score: This measures how well-separated the clusters are. Silhouette score for a point is defined as (b - a) / max(a, b), where a is the average distance to other points in the same cluster (cohesion) and b is the average distance to points in the nearest other cluster (separation) ￼. The score ranges from -1 to 1; higher means the point is in the right cluster (well separated from others) ￼. We often use the average silhouette over all points to evaluate the clustering quality for a given K. A higher average silhouette indicates more distinct clustering structure.\n\nLet’s compute these metrics for K = 2 through 7 and see which K might be best for our penguin data:\n\n\nWCSS for K=2..7: [20949.7853112782, 13859.543763777465, 9587.135276652696, 7424.8815080702425, 6332.78650281499, 5257.088985919957]\nSilhouette scores for K=2..7: [0.6117940477662409, 0.48062618684371067, 0.4448839684032104, 0.42719392387268307, 0.41278625329909713, 0.4307326685776095]\n\n\nPlotting these values:\n\n\n\n\n\n\n\n\n\nEvaluating different cluster counts on the penguin data. Left: WCSS (sum of squared distances within clusters) for K=2 to 7. Right: Average silhouette score for K=2 to 7. Higher silhouette is better.\nFrom the above, WCSS steadily decreases as K increases (as expected), and there’s a bit of a knee around K=3-4 (the “elbow” is not super sharp here). The silhouette score peaks at K=2 (about 0.61) and then drops for higher K, hitting a low around 0.41–0.44 for K=4-6, with a slight uptick at K=7. A high silhouette means clusters are well separated, so this suggests that using 2 clusters yields the clearest separation in this 2D feature space.\nThis is interesting because we know there are 3 species. What’s happening is that two of the species (Adelie and Chinstrap) have very similar bill/flipper measurements that overlap a lot, so the algorithm doesn’t find a clear separation between them and favors combining them into one cluster. The Gentoo penguins, on the other hand, form a very distinct cluster. Thus, K=2 gives two well-separated clusters (basically “Gentoo” and “Adelie+Chinstrap”), which yields a higher silhouette score than K=3 where the algorithm is forced to split the Adelie/Chinstrap group and ends up with one weaker separation. In a real analysis, we’d consider this trade-off: do we want 3 clusters to match known species, or 2 clusters that are most distinct in these features? There’s no single correct answer – it depends on the goal. But this illustrates how WCSS and silhouette can guide the choice of K."
  },
  {
    "objectID": "portfolio/clustering-classification/Assignment_4.html#k-nearest-neighbors-knn",
    "href": "portfolio/clustering-classification/Assignment_4.html#k-nearest-neighbors-knn",
    "title": "Clustering & Classification Methods",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\nNext, let’s switch gears to a supervised learning method: K-Nearest Neighbors. KNN is one of the simplest classification algorithms. The basic idea: to predict the class of a new point, look at the “k” closest points in the training data and take a majority vote. In other words, the label is determined by the plurality of the labels of its k nearest neighbors in the training set. If k=5, the 5 closest neighbors’ labels are used – if 3 of them are class A and 2 are class B, the new point is classified as A (majority wins). KNN makes no assumptions about data distribution (it’s a non-parametric, instance-based method), and it’s considered a “lazy” learner because it doesn’t build an explicit model; it just stores the training instances and defers computation to query time.\nChoosing k: The parameter k controls the model’s complexity. A small k (e.g. 1) means the classifier can be very flexible and even noisy – essentially memorizing the training data (low bias, high variance). A large k means we smooth over more neighbors, making the classifier more stable but potentially less sensitive to local patterns (high bias, low variance). We’ll see this trade-off in action with a synthetic example.\n\nSynthetic Dataset with a Nonlinear Boundary\nTo illustrate KNN, we’ll create a 2D synthetic dataset for binary classification. We want something non-linear to show KNN’s strength at capturing complex boundaries. We generate 100 random points uniformly inside the square \\[\n(x_1,x_2)\\in[-3,3]\n\\]\nThe true boundary is the wiggly curve\n\\[\nx_2 \\;=\\; \\sin(4x_1)+x_1,\n\\]\nso we label a point class 1 when \\(x_2\\) lies above the curve and class 0 otherwise. This produces two interwoven regions separated by a sine wave, giving KNN a genuinely non‑linear problem.\n\n\nTraining set class counts: [49 51]\nTest set class counts: [53 47]\n\n\nWe have created a training set of 100 points and a test set of 100 points. Let’s visualize the training data and the true boundary:\n\n\n\n\n\n\n\n\n\n*Synthetic dataset: 100 training points (blue = class 0, red = class 1) in the square \\([-3,3]^2\\)\nThe green dashed line is the true boundary\n\\[\nx_2 \\;=\\; \\sin(4x_1)+x_1,\n\\]\nYou can see that class 0 (blue) occupies the area below the sine curve, and class 1 (red) is above the curve. The boundary wiggles up and down. No linear model could classify this perfectly, but KNN should do well given enough neighbors.\n\n\nImplementing KNN from Scratch\nThe KNN algorithm for classification is straightforward to implement. For each query (test point), we need to find the distances to all training points, pick the k nearest, and take a majority vote of their labels. A basic implementation might be O(n) per query (where n is number of training points), which can be slow for large datasets, but for our data size it’s fine.\nWe’ll write a function knn_predict(X_train, y_train, X_test, k) that returns predicted labels for the X_test points:\n\nimport math\n\n\ndef knn_predict(X_train, y_train, X_test, k):\n    predictions = []\n    for x in X_test:\n        # Compute distance from x to all points in X_train (Euclidean)\n        distances = np.linalg.norm(X_train - x, axis=1)\n        # Find the indices of the k nearest neighbors\n        nn_idx = np.argsort(distances)[:k]\n        nn_labels = y_train[nn_idx]\n        # Majority vote: the predicted class is the mode of the neighbor labels\n        # For binary labels 0/1, we can just take the mean and round it\n        vote = 1 if nn_labels.mean() &gt;= 0.5 else 0\n        predictions.append(vote)\n    return np.array(predictions)\n\n\n# Try out the KNN predictor for k=5 on a few test points\ny_pred_test_k5 = knn_predict(X_train, y_train, X_test[:5], k=5)\nprint(\"True labels:     \", y_test[:5])\nprint(\"Predicted labels:\", y_pred_test_k5)\n\nTrue labels:      [1 1 0 1 1]\nPredicted labels: [1 1 0 1 1]\n\n\nIn practice, one can use spatial data structures (KD-trees, ball trees) for large datasets, but we’re keeping it simple. The majority vote is implemented by averaging the 0/1 labels and rounding – this works because if a majority are 1s, the average &gt; 0.5, otherwise it’s &lt; 0.5 (for multiclass, you’d do something like collections.Counter or np.bincount to get the mode).\nLet’s see how our scratch KNN performs and verify it against scikit-learn’s KNN classifier:\n\n\nOur KNN vs sklearn KNN match: True\nTest accuracy (k=5) scratch: 0.900\nTest accuracy (k=5) sklearn: 0.900\n\n\nIf we run this, we find that our implementation’s predictions exactly match scikit-learn’s for k=5 (and indeed they should for any k). The accuracy on the test set for k=5 is around 0.9 (90%). It’s good to confirm that our scratch code is working properly.\n\n\nFinding the Best k (Accuracy vs. k)\nFinally, let’s see how the choice of k affects the performance on this problem. We’ll evaluate k from 1 up to 30 and record the classification accuracy on the test set for each:\n\n\nBest k = 1 with test accuracy = 0.920\n\n\nPlotting these accuracies as a function of k:\n\n\n\n\n\n\n\n\n\nThe exact numbers may vary slightly with random seed, but the trend is clear: the highest accuracy in this case is about 92 % for the best k, then it drops off as k increases. The plot of accuracy vs k would show a peak at a very low k, then a gradual decline.\nWhy does this happen? Our synthetic data has a very clean, deterministic boundary (no noise in labels given the features). A very flexible model like 1-nearest-neighbor can trace that wavy boundary almost perfectly – in fact, 1-NN yields about 92 % accuracy on the test! It slightly overfits to the discrete sample (if we had infinite data, 1-NN would eventually perfectly model the true boundary). Using a larger k smooths the decision boundary. For instance, at k=5 we saw accuracy around 0.9 (90%) – some fine detail is lost, as the model sometimes errs on points near the boundary by averaging in neighbors from the other side of the wave. By k=30, accuracy drops further, because with such a large neighborhood, the classifier is getting too biased – it’s oversmoothing and doesn’t capture the wiggles at all (in the extreme, if you used k equal to the entire training set, it would always predict the majority class, ignoring the input features altogether!). This illustrates the bias-variance trade-off in KNN: small k = low bias, high variance; large k = high bias, low variance. The optimal k is often somewhere in between, and one usually uses cross-validation to find it in practice. In our case, since the data has no label noise, the best performance is at the most flexible end (k=1 or 3).\nTo visualize what KNN is doing, here’s a depiction of the decision boundary learned by our model for two different k values: - For k=1, the decision boundary will essentially trace around every red point (each training point defines its own region via a Voronoi partition). It will be very jagged and pass through narrow gaps – perfectly fitting the training data, but potentially too wiggly for noisy data. - For k=15 (for example), the decision boundary will be much smoother – small irregularities are averaged out, and only broader trends remain. It might misclassify some points that lie in small “bays” of the opposite class, as it favors the overall majority in a larger neighborhood ￼.\nIn our synthetic example, the boundary was truly sinusoidal, so a very flexible model wins. If we had added noise or outliers, a slightly larger k would likely be better to avoid chasing that noise. This is a great demonstration of how KNN’s parameter k influences the model complexity.\n\n\nConclusion\nWe’ve created a step-by-step walkthrough of K-Means and K-Nearest Neighbors, implementing both from scratch and validating against library implementations. For K-Means, we clustered penguins by their physical measurements and used WCSS and silhouette analysis to reason about the appropriate number of clusters. For KNN, we built a synthetic classification problem with a non-linear boundary and saw how the choice of neighbors affects performance. Along the way, we visualized the clustering process (with an animated centroid update plot) and the classification boundary (implicitly, through the accuracy vs k behavior and discussion).\nKey Takeaways:\n\nK-Means is an unsupervised clustering algorithm that iteratively assigns points to the nearest centroid and updates centroids to the mean of points ￼. It aims to minimize within-cluster variance (WCSS). Choosing the number of clusters K is crucial – methods like the elbow plot and silhouette analysis help evaluate cluster quality.\nK-Nearest Neighbors is a simple yet powerful supervised learning method that classifies points based on the majority label of their nearest neighbors. It is easy to implement and makes no distribution assumptions (non-parametric). The parameter k controls the bias-variance trade-off: smaller k can capture fine-grained patterns but may overfit, while larger k smooths out noise but may miss details.\nBoth algorithms are intuitive K-Means finds natural groupings in data, and KNN makes predictions by “consulting” nearby examples. Despite their simplicity, they are effective for a wide range of problems and are excellent for building intuition about clustering and classification.\n\nHopefully, this walkthrough helped clarify how K-Means and KNN work. Feel free to experiment with different parameters or datasets – tweak the number of clusters, or try KNN on a different boundary shape – to further solidify your understanding. Happy learning!"
  },
  {
    "objectID": "Other/Assignment_1_Stuff/File_Conversion.html",
    "href": "Other/Assignment_1_Stuff/File_Conversion.html",
    "title": "Juan Hernández Guizar",
    "section": "",
    "text": "import pandas as pd\n\nkarlan_list_2007_csv = pd.read_stata(\"karlan_list_2007.dta\")\n\nkarlan_list_2007_csv.to_csv(\"karlan_list_2007_csv.csv\", index=False)"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Juan’s Resume",
    "section": "",
    "text": "Last updated: 2025-29-12\nDownload PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Juan Hernández Guizar",
    "section": "",
    "text": "Juan Hernández Guizar is an Aerospace Systems Engineer at Blue Origin, leading digital transformation and model-based systems engineering efforts on cislunar and beyond focused programs. Passionate about inclusive leadership and technical excellence, Juan also champions community-building."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Juan Hernández Guizar",
    "section": "Education",
    "text": "Education\nUC San Diego | San Diego, CA\nM.S. in Business Analytics (Dec 2025)\nUC Merced | Merced, CA\nB.S. in Mechanical Engineering (May 2018)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Juan Hernández Guizar",
    "section": "Experience",
    "text": "Experience\nBlue Origin | Aerospace Systems Engineer\nEl Segundo, CA | 2023 – Present\nLeading digital ecosystem initiatives and systems modeling for Blue Origins Aerobrake.\nMercury Systems | Systems Engineering Manager\nTorrance, CA | 2021 – 2022\nManaged 10 engineers across multiple aerospace programs.\nRockwell Collins | Systems Engineer\nCedar Rapids, IA | 2018 – 2021\nScrum master and test lead for Embraer and Airbus platforms."
  },
  {
    "objectID": "portfolio/patent-airbnb-analysis/Assignment_2.html",
    "href": "portfolio/patent-airbnb-analysis/Assignment_2.html",
    "title": "Patent Success & Airbnb Demand Analysis",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo start we will review the first 5 data points collected in a table format:\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nBelow is the distribution of patents among Blueprinty customers vs non-customers. The histogram shows that Blueprinty customers (light orange bars) tend to have more patents on average than non-customers (light blue bars).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have approximately 4.13 patents over 5 years, compared to about 3.47 for non-customers. While this naive comparison suggests customers produce more patents, we must consider that Blueprinty’s customers may differ systematically in other ways (e.g. perhaps they are older firms or clustered in certain regions).\nLet’s examine the age and regional composition of customers vs non-customers.\n\n\n\n\n\n\n\n\n\nBlueprinty customers have a slightly higher mean age since incorporation (about 26.9 years) than non-customers (26.1 years), but the age distributions largely overlap (both groups are typically 20–30 years old, with only minor differences). This suggests that firm age might not differ dramatically by customer status, though we will account for age in the analysis.\nRegionally, there are stark differences in who adopts Blueprinty’s software.\n\n\n\n\n\n\n\n\n\nCounts of firms by region and Blueprinty customer status. In the Northeast region, the green bar (Blueprinty customers) is higher than the blue bar (non-customers), indicating a large share of Blueprinty’s users are in the Northeast. In contrast, in all other regions (Midwest, South, Southwest, Northwest) the majority of firms are non-customers. This reveals that Blueprinty’s customer base is heavily concentrated in the Northeast, which suggests potential selection bias by region.\nIndeed, about 68% of Blueprinty’s customers are located in the Northeast, whereas only ~27% of non-customer firms are in the Northeast. Other regions (Midwest, South, Southwest, Northwest) are under-represented among customers relative to non-customers. This imbalance means any raw difference in patent counts could partly reflect regional effects. In summary, Blueprinty customers tend to have slightly older firms (though age differences are minor) and are much more likely to be in the Northeast region. We will need to control for these factors when analyzing the effect of Blueprinty’s software on patent output.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow, \\(Y_i\\) is the patent count for firm \\(i\\) and \\(\\lambda\\) is the average number of patents per firm in five years.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(n\\)\nTotal number of firms (length of \\(\\mathbf y\\))\n\n\n\\(\\lambda\\)\nPoisson rate parameter — the mean (and variance) of the distribution\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(\\mathcal L(\\lambda;\\mathbf y)\\)\nLikelihood of the entire dataset, given \\(\\lambda\\)\n\n\n\\(\\ell(\\lambda)\\)\nLog-likelihood, \\(\\ell(\\lambda)=\\log \\mathcal L(\\lambda;\\mathbf y)\\)\n\n\n\n\\[\nP\\!\\bigl(Y_i=y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i=0,1,2,\\dots\n\\]\n\\[\n\\boxed{\n  \\mathcal L(\\lambda;\\mathbf y)=\n    e^{-n\\lambda}\\,\n    \\lambda^{\\sum_{i=1}^{n} y_i}\\!\n    \\Big/\n    \\prod_{i=1}^{n} y_i!\n}\n\\qquad\n\\boxed{\n  \\ell(\\lambda)=\n    \\sum_{i=1}^{n}\\!\n      \\bigl(\n        y_i\\log\\lambda-\\lambda-\\log y_i!\n      \\bigr)\n}\n\\]\nPutting all counts into one vector lets us write the likelihood compactly and pass the entire dataset to a single log-likelihood function. In the code below that function is called loglik_poisson. It follows the “parameter-vector” convention most optimisers expect: the one unknown, lambda, is stored as theta[0]. This style makes the function future-proof—if we later add more parameters we can just extend the theta vector without rewriting the optimiser call.\n\nimport numpy as np, math\n\n\ndef loglik_poisson(theta, y):\n    \"\"\"\n    Poisson log-likelihood\n\n    Parameters\n    ----------\n    theta : 1-element array-like\n        theta[0] = λ  (must be &gt; 0)\n    y     : 1-D numpy array of non-negative integers\n\n    Returns\n    -------\n    float\n        Scalar log-likelihood ℓ(λ)\n    \"\"\"\n    lam = float(theta[0])  # ← parallels 'mu &lt;- theta[1]'\n    if lam &lt;= 0:\n        return -np.inf  # guard just like s2&gt;0 in Normal case\n\n    n = y.size\n    # ll = Σ(y_i log λ) − n λ − Σ log(y_i!)\n    ll = np.sum(y * np.log(lam)) - n * lam - np.sum([math.lgamma(k + 1) for k in y])\n    return ll\n\nThe curve below shows how the log-likelihood changes as we slide λ across plausible values. It rises steeply, flattens, and then falls—peaking (unsurprisingly) right where λ equals the sample mean (~3.7 patents). That single highest point is the Maximum-Likelihood Estimate: the value of λ that makes the observed patent counts most probable under a Poisson model.\n\n\n\n\n\n\n\n\n\n\\[\nP\\!\\bigl(Y_i = y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i = 0,1,2,\\dots\n\\]\nDifferentiating the log-likelihood\n\\[\n\\ell(\\lambda)=\\sum_{i=1}^{n}\n  \\bigl(\n    y_i\\log\\lambda-\\lambda-\\log y_i!\n  \\bigr)\n\\]\nwith respect to () and setting the derivative to zero gives\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n  \\;=\\;\n  \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}-n\n  \\;=\\;0\n  \\;\\Longrightarrow\\;\n  \\boxed{\\hat\\lambda=\\bar y}\n\\]\nso the maximum-likelihood estimate is nothing more than the sample mean of the counts. The first code cell reflects that algebra exactly: y.mean() is computed and printed as the Analytic MLE, which for our data equals 3.6847 patents per firm.\n\n\nAnalytic MLE   λ̂ = 3.6847\n\n\n\n\nOptimiser MLE  λ̂ = 3.6847\n\n\nThe second cell tackles the same task numerically. scipy.optimize.minimize_scalar is instructed to minimise the negative log-likelihood (equivalently maximise ()), searching over the interval ([10^{-4},10]). Because the optimiser treats () as a scalar, we wrap it in a one-element list when passing it to loglik_poisson. After a quick line search it returns an Optimiser MLE of 3.6847, matching the analytic result to four decimal places—strong confirmation that the calculus and the numerical optimisation tell the same story.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\nA covariate (sometimes called a feature or explanatory variable) is simply an observed attribute we believe helps explain the outcome. Here our covariates are age, age ² (to capture curvature), a set of region dummies, and a binary flag for Blueprinty customer status. By stacking these in a matrix \\(X\\) and multiplying by a coefficient vector \\(\\boldsymbol\\beta\\), we let each firm have its own mean rate \\(\\lambda_i=\\exp(X_i^{!\\top}\\boldsymbol\\beta)\\)—the exponential ensures every \\(\\lambda_i\\) stays positive.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(X_i\\)\nRow vector of covariates for firm \\(i\\) (intercept, age, age\\(^{2}\\), region dummies, Blueprinty flag)\n\n\n\\(\\boldsymbol\\beta\\)\nColumn vector of coefficients (one per covariate)\n\n\n\\(\\lambda_i\\)\nMean patents for firm \\(i\\): \\(\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\\)\n\n\n\\(n\\)\nTotal number of firms (rows of \\(X\\))\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(X\\)\nDesign matrix that stacks all \\(X_i\\) rows\n\n\n\\(\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\nLikelihood of the entire dataset, given \\(\\boldsymbol\\beta\\)\n\n\n\\(\\ell(\\boldsymbol\\beta)\\)\nLog-likelihood, \\(\\ell(\\boldsymbol\\beta)=\\log \\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\n\n\n\nExpanding from a constant‐rate model to Poisson regression swaps the single parameter \\(\\lambda\\) for a whole vector of coefficients \\(\\boldsymbol\\beta\\).\nEach firm now gets its own mean rate through the inverse-link function\n\\(\\lambda_i=\\exp(X_i^{\\!\\top}\\boldsymbol\\beta)\\), guaranteeing positivity while letting the linear predictor \\(X_i^{\\!\\top}\\boldsymbol\\beta\\) wander over the real line.\nThe covariate matrix \\(X\\) holds an intercept, age, age², a set of region dummies, and a Blueprinty-customer flag, so any of those characteristics can nudge the expected patent count up or down.\n\\[\nY_i \\,\\bigl|\\, X_i \\;\\sim\\; \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i \\;=\\; \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr),\n\\qquad i = 1,\\dots,n.\n\\]\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  \\;=\\;\n  \\prod_{i=1}^{n}\n    \\frac{e^{-\\lambda_i}\\,\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  \\;=\\;\n  \\sum_{i=1}^{n}\n    \\Bigl(\n      Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n      \\;-\\;\n      \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n      \\;-\\;\n      \\log Y_i!\n    \\Bigr).\n\\]\nThe code block that follows translates this math into Python.\nloglik_poisson_reg(beta, y, X) now takes both the response vector and the covariate matrix, computes the linear predictor \\(X\\boldsymbol\\beta\\), exponentiates to obtain \\(\\boldsymbol\\lambda\\), and returns the scalar log-likelihood. Passing that function to an optimiser (e.g. scipy.optimize.minimize) yields the maximum-likelihood estimates for the full coefficient vector \\(\\boldsymbol\\beta\\).\n\nimport numpy as np, math\n\n\ndef loglik_poisson_reg(beta, y, X):\n    \"\"\"\n    Poisson regression log-likelihood.\n\n    beta : 1-D array, length p           (coefficients)\n    y    : 1-D array, length n           (counts)\n    X    : 2-D array, shape (n, p)       (covariate matrix)\n\n    Returns\n    -------\n    float : scalar log-likelihood ℓ(β)\n    \"\"\"\n    eta = X @ beta  # linear predictor  η = Xβ  (shape n)\n    lam = np.exp(eta)  # inverse link  λ = exp(η)\n    if np.any(lam &lt;= 0):\n        return -np.inf  # numerical safety\n    ll = np.sum(y * eta - lam - [math.lgamma(k + 1) for k in y])\n    return ll\n\nWith this function we can now hand the entire β vector to an optimiser (e.g. scipy.optimize.minimize) to obtain maximum-likelihood estimates, just as we did for the single-parameter case—only now the model flexes with age, geography, and Blueprinty adoption.\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  = \\prod_{i=1}^{n}\n      \\frac{e^{-\\lambda_i}\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  = \\sum_{i=1}^{n}\n      \\Bigl(\n        Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n        - \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n        - \\log Y_i!\n      \\Bigr).\n\\]\nA Hessian is the log-likelihood’s curvature map. Picture the likelihood surface as a hill; the Hessian tells us how sharply that hill drops away in every parameter direction. Formally\n\\[\nH(\\hat{\\boldsymbol\\beta})\n  = -\n    \\frac{\\partial^2\\ell(\\boldsymbol\\beta)}\n         {\\partial\\boldsymbol\\beta\\,\\partial\\boldsymbol\\beta^{\\!\\top}}\n  \\Biggr\\rvert_{\\;\\boldsymbol\\beta=\\hat{\\boldsymbol\\beta}},\n\\]\nand its negative inverse is the large-sample covariance of the MLEs, so ((_j)=).\n\n\n\n\n\n\n\n\n\nCoefficient\nStd.Error\n\n\n\n\nconst\n-0.5089\n0.1870\n\n\nage\n0.1486\n0.0140\n\n\nage2\n-0.2970\n0.0260\n\n\niscustomer\n0.2076\n0.0330\n\n\nregion_Northeast\n0.0292\n0.0465\n\n\nregion_Northwest\n-0.0176\n0.0572\n\n\nregion_South\n0.0566\n0.0558\n\n\nregion_Southwest\n0.0506\n0.0501\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nFri, 02 Jan 2026\nDeviance:\n2143.3\n\n\nTime:\n21:07:22\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.2970\n0.026\n-11.513\n0.000\n-0.348\n-0.246\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nAfter rescaling age² and bounding the search, the Poisson regression converges cleanly. Key take-aways:\n\nAge (+) and Age² (−) form a concave pattern—patenting rises, peaks mid-20s, then tapers.\nRegion dummies shrink toward zero once we explicitly control for Blueprinty usage, implying geography itself isn’t the driver; the earlier Northeast spike simply reflected the high concentration of customers there.\nBlueprinty customer (+0.208) yields ≈ 23 % higher expected patents, highly significant even after all other controls.\n\nHand-rolled MLEs, Hessian-based standard errors, and statsmodels GLM all agree, giving us confidence in the estimates and the narrative.\nTo translate the log-coefficient into “extra patents,” we ran a simple counter-factual:\n\nX₀: keep every firm’s age and region but set iscustomer = 0.\n\nX₁: identical matrix but flip iscustomer = 1.\n\nPredict \\(\\hat y_0=\\exp(X_0\\hat\\beta)\\) and \\(\\hat y_1=\\exp(X_1\\hat\\beta)\\).\n\nTake the firm-by-firm difference \\(\\hat y_1-\\hat y_0\\) and average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-customer\nBlueprinty\nLift\n\n\n\n\n0\n3.44\n4.23\n0.79\n\n\n\n\n\n\n\nThe result: +0.82 patents per firm over five years, about a 22 % lift relative to the baseline mean. The density plot below shows most firms gain between 0.5 and 1.1 extra patents, with a long but light right tail for the largest firms.\n\n\n\nAfter accounting for firm age and regional differences, using Blueprinty still delivers about one additional granted patent every five years. For most engineering shops that’s a solid, tangible boost—enough to nudge a “nice idea” into a fully protected asset on the balance sheet."
  },
  {
    "objectID": "portfolio/patent-airbnb-analysis/Assignment_2.html#blueprinty-case-study",
    "href": "portfolio/patent-airbnb-analysis/Assignment_2.html#blueprinty-case-study",
    "title": "Patent Success & Airbnb Demand Analysis",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo start we will review the first 5 data points collected in a table format:\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nBelow is the distribution of patents among Blueprinty customers vs non-customers. The histogram shows that Blueprinty customers (light orange bars) tend to have more patents on average than non-customers (light blue bars).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have approximately 4.13 patents over 5 years, compared to about 3.47 for non-customers. While this naive comparison suggests customers produce more patents, we must consider that Blueprinty’s customers may differ systematically in other ways (e.g. perhaps they are older firms or clustered in certain regions).\nLet’s examine the age and regional composition of customers vs non-customers.\n\n\n\n\n\n\n\n\n\nBlueprinty customers have a slightly higher mean age since incorporation (about 26.9 years) than non-customers (26.1 years), but the age distributions largely overlap (both groups are typically 20–30 years old, with only minor differences). This suggests that firm age might not differ dramatically by customer status, though we will account for age in the analysis.\nRegionally, there are stark differences in who adopts Blueprinty’s software.\n\n\n\n\n\n\n\n\n\nCounts of firms by region and Blueprinty customer status. In the Northeast region, the green bar (Blueprinty customers) is higher than the blue bar (non-customers), indicating a large share of Blueprinty’s users are in the Northeast. In contrast, in all other regions (Midwest, South, Southwest, Northwest) the majority of firms are non-customers. This reveals that Blueprinty’s customer base is heavily concentrated in the Northeast, which suggests potential selection bias by region.\nIndeed, about 68% of Blueprinty’s customers are located in the Northeast, whereas only ~27% of non-customer firms are in the Northeast. Other regions (Midwest, South, Southwest, Northwest) are under-represented among customers relative to non-customers. This imbalance means any raw difference in patent counts could partly reflect regional effects. In summary, Blueprinty customers tend to have slightly older firms (though age differences are minor) and are much more likely to be in the Northeast region. We will need to control for these factors when analyzing the effect of Blueprinty’s software on patent output.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow, \\(Y_i\\) is the patent count for firm \\(i\\) and \\(\\lambda\\) is the average number of patents per firm in five years.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(n\\)\nTotal number of firms (length of \\(\\mathbf y\\))\n\n\n\\(\\lambda\\)\nPoisson rate parameter — the mean (and variance) of the distribution\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(\\mathcal L(\\lambda;\\mathbf y)\\)\nLikelihood of the entire dataset, given \\(\\lambda\\)\n\n\n\\(\\ell(\\lambda)\\)\nLog-likelihood, \\(\\ell(\\lambda)=\\log \\mathcal L(\\lambda;\\mathbf y)\\)\n\n\n\n\\[\nP\\!\\bigl(Y_i=y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i=0,1,2,\\dots\n\\]\n\\[\n\\boxed{\n  \\mathcal L(\\lambda;\\mathbf y)=\n    e^{-n\\lambda}\\,\n    \\lambda^{\\sum_{i=1}^{n} y_i}\\!\n    \\Big/\n    \\prod_{i=1}^{n} y_i!\n}\n\\qquad\n\\boxed{\n  \\ell(\\lambda)=\n    \\sum_{i=1}^{n}\\!\n      \\bigl(\n        y_i\\log\\lambda-\\lambda-\\log y_i!\n      \\bigr)\n}\n\\]\nPutting all counts into one vector lets us write the likelihood compactly and pass the entire dataset to a single log-likelihood function. In the code below that function is called loglik_poisson. It follows the “parameter-vector” convention most optimisers expect: the one unknown, lambda, is stored as theta[0]. This style makes the function future-proof—if we later add more parameters we can just extend the theta vector without rewriting the optimiser call.\n\nimport numpy as np, math\n\n\ndef loglik_poisson(theta, y):\n    \"\"\"\n    Poisson log-likelihood\n\n    Parameters\n    ----------\n    theta : 1-element array-like\n        theta[0] = λ  (must be &gt; 0)\n    y     : 1-D numpy array of non-negative integers\n\n    Returns\n    -------\n    float\n        Scalar log-likelihood ℓ(λ)\n    \"\"\"\n    lam = float(theta[0])  # ← parallels 'mu &lt;- theta[1]'\n    if lam &lt;= 0:\n        return -np.inf  # guard just like s2&gt;0 in Normal case\n\n    n = y.size\n    # ll = Σ(y_i log λ) − n λ − Σ log(y_i!)\n    ll = np.sum(y * np.log(lam)) - n * lam - np.sum([math.lgamma(k + 1) for k in y])\n    return ll\n\nThe curve below shows how the log-likelihood changes as we slide λ across plausible values. It rises steeply, flattens, and then falls—peaking (unsurprisingly) right where λ equals the sample mean (~3.7 patents). That single highest point is the Maximum-Likelihood Estimate: the value of λ that makes the observed patent counts most probable under a Poisson model.\n\n\n\n\n\n\n\n\n\n\\[\nP\\!\\bigl(Y_i = y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i = 0,1,2,\\dots\n\\]\nDifferentiating the log-likelihood\n\\[\n\\ell(\\lambda)=\\sum_{i=1}^{n}\n  \\bigl(\n    y_i\\log\\lambda-\\lambda-\\log y_i!\n  \\bigr)\n\\]\nwith respect to () and setting the derivative to zero gives\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n  \\;=\\;\n  \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}-n\n  \\;=\\;0\n  \\;\\Longrightarrow\\;\n  \\boxed{\\hat\\lambda=\\bar y}\n\\]\nso the maximum-likelihood estimate is nothing more than the sample mean of the counts. The first code cell reflects that algebra exactly: y.mean() is computed and printed as the Analytic MLE, which for our data equals 3.6847 patents per firm.\n\n\nAnalytic MLE   λ̂ = 3.6847\n\n\n\n\nOptimiser MLE  λ̂ = 3.6847\n\n\nThe second cell tackles the same task numerically. scipy.optimize.minimize_scalar is instructed to minimise the negative log-likelihood (equivalently maximise ()), searching over the interval ([10^{-4},10]). Because the optimiser treats () as a scalar, we wrap it in a one-element list when passing it to loglik_poisson. After a quick line search it returns an Optimiser MLE of 3.6847, matching the analytic result to four decimal places—strong confirmation that the calculus and the numerical optimisation tell the same story.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\nA covariate (sometimes called a feature or explanatory variable) is simply an observed attribute we believe helps explain the outcome. Here our covariates are age, age ² (to capture curvature), a set of region dummies, and a binary flag for Blueprinty customer status. By stacking these in a matrix \\(X\\) and multiplying by a coefficient vector \\(\\boldsymbol\\beta\\), we let each firm have its own mean rate \\(\\lambda_i=\\exp(X_i^{!\\top}\\boldsymbol\\beta)\\)—the exponential ensures every \\(\\lambda_i\\) stays positive.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(X_i\\)\nRow vector of covariates for firm \\(i\\) (intercept, age, age\\(^{2}\\), region dummies, Blueprinty flag)\n\n\n\\(\\boldsymbol\\beta\\)\nColumn vector of coefficients (one per covariate)\n\n\n\\(\\lambda_i\\)\nMean patents for firm \\(i\\): \\(\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\\)\n\n\n\\(n\\)\nTotal number of firms (rows of \\(X\\))\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(X\\)\nDesign matrix that stacks all \\(X_i\\) rows\n\n\n\\(\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\nLikelihood of the entire dataset, given \\(\\boldsymbol\\beta\\)\n\n\n\\(\\ell(\\boldsymbol\\beta)\\)\nLog-likelihood, \\(\\ell(\\boldsymbol\\beta)=\\log \\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\n\n\n\nExpanding from a constant‐rate model to Poisson regression swaps the single parameter \\(\\lambda\\) for a whole vector of coefficients \\(\\boldsymbol\\beta\\).\nEach firm now gets its own mean rate through the inverse-link function\n\\(\\lambda_i=\\exp(X_i^{\\!\\top}\\boldsymbol\\beta)\\), guaranteeing positivity while letting the linear predictor \\(X_i^{\\!\\top}\\boldsymbol\\beta\\) wander over the real line.\nThe covariate matrix \\(X\\) holds an intercept, age, age², a set of region dummies, and a Blueprinty-customer flag, so any of those characteristics can nudge the expected patent count up or down.\n\\[\nY_i \\,\\bigl|\\, X_i \\;\\sim\\; \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i \\;=\\; \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr),\n\\qquad i = 1,\\dots,n.\n\\]\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  \\;=\\;\n  \\prod_{i=1}^{n}\n    \\frac{e^{-\\lambda_i}\\,\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  \\;=\\;\n  \\sum_{i=1}^{n}\n    \\Bigl(\n      Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n      \\;-\\;\n      \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n      \\;-\\;\n      \\log Y_i!\n    \\Bigr).\n\\]\nThe code block that follows translates this math into Python.\nloglik_poisson_reg(beta, y, X) now takes both the response vector and the covariate matrix, computes the linear predictor \\(X\\boldsymbol\\beta\\), exponentiates to obtain \\(\\boldsymbol\\lambda\\), and returns the scalar log-likelihood. Passing that function to an optimiser (e.g. scipy.optimize.minimize) yields the maximum-likelihood estimates for the full coefficient vector \\(\\boldsymbol\\beta\\).\n\nimport numpy as np, math\n\n\ndef loglik_poisson_reg(beta, y, X):\n    \"\"\"\n    Poisson regression log-likelihood.\n\n    beta : 1-D array, length p           (coefficients)\n    y    : 1-D array, length n           (counts)\n    X    : 2-D array, shape (n, p)       (covariate matrix)\n\n    Returns\n    -------\n    float : scalar log-likelihood ℓ(β)\n    \"\"\"\n    eta = X @ beta  # linear predictor  η = Xβ  (shape n)\n    lam = np.exp(eta)  # inverse link  λ = exp(η)\n    if np.any(lam &lt;= 0):\n        return -np.inf  # numerical safety\n    ll = np.sum(y * eta - lam - [math.lgamma(k + 1) for k in y])\n    return ll\n\nWith this function we can now hand the entire β vector to an optimiser (e.g. scipy.optimize.minimize) to obtain maximum-likelihood estimates, just as we did for the single-parameter case—only now the model flexes with age, geography, and Blueprinty adoption.\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  = \\prod_{i=1}^{n}\n      \\frac{e^{-\\lambda_i}\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  = \\sum_{i=1}^{n}\n      \\Bigl(\n        Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n        - \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n        - \\log Y_i!\n      \\Bigr).\n\\]\nA Hessian is the log-likelihood’s curvature map. Picture the likelihood surface as a hill; the Hessian tells us how sharply that hill drops away in every parameter direction. Formally\n\\[\nH(\\hat{\\boldsymbol\\beta})\n  = -\n    \\frac{\\partial^2\\ell(\\boldsymbol\\beta)}\n         {\\partial\\boldsymbol\\beta\\,\\partial\\boldsymbol\\beta^{\\!\\top}}\n  \\Biggr\\rvert_{\\;\\boldsymbol\\beta=\\hat{\\boldsymbol\\beta}},\n\\]\nand its negative inverse is the large-sample covariance of the MLEs, so ((_j)=).\n\n\n\n\n\n\n\n\n\nCoefficient\nStd.Error\n\n\n\n\nconst\n-0.5089\n0.1870\n\n\nage\n0.1486\n0.0140\n\n\nage2\n-0.2970\n0.0260\n\n\niscustomer\n0.2076\n0.0330\n\n\nregion_Northeast\n0.0292\n0.0465\n\n\nregion_Northwest\n-0.0176\n0.0572\n\n\nregion_South\n0.0566\n0.0558\n\n\nregion_Southwest\n0.0506\n0.0501\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nFri, 02 Jan 2026\nDeviance:\n2143.3\n\n\nTime:\n21:07:22\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.2970\n0.026\n-11.513\n0.000\n-0.348\n-0.246\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nAfter rescaling age² and bounding the search, the Poisson regression converges cleanly. Key take-aways:\n\nAge (+) and Age² (−) form a concave pattern—patenting rises, peaks mid-20s, then tapers.\nRegion dummies shrink toward zero once we explicitly control for Blueprinty usage, implying geography itself isn’t the driver; the earlier Northeast spike simply reflected the high concentration of customers there.\nBlueprinty customer (+0.208) yields ≈ 23 % higher expected patents, highly significant even after all other controls.\n\nHand-rolled MLEs, Hessian-based standard errors, and statsmodels GLM all agree, giving us confidence in the estimates and the narrative.\nTo translate the log-coefficient into “extra patents,” we ran a simple counter-factual:\n\nX₀: keep every firm’s age and region but set iscustomer = 0.\n\nX₁: identical matrix but flip iscustomer = 1.\n\nPredict \\(\\hat y_0=\\exp(X_0\\hat\\beta)\\) and \\(\\hat y_1=\\exp(X_1\\hat\\beta)\\).\n\nTake the firm-by-firm difference \\(\\hat y_1-\\hat y_0\\) and average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-customer\nBlueprinty\nLift\n\n\n\n\n0\n3.44\n4.23\n0.79\n\n\n\n\n\n\n\nThe result: +0.82 patents per firm over five years, about a 22 % lift relative to the baseline mean. The density plot below shows most firms gain between 0.5 and 1.1 extra patents, with a long but light right tail for the largest firms.\n\n\n\nAfter accounting for firm age and regional differences, using Blueprinty still delivers about one additional granted patent every five years. For most engineering shops that’s a solid, tangible boost—enough to nudge a “nice idea” into a fully protected asset on the balance sheet."
  },
  {
    "objectID": "portfolio/patent-airbnb-analysis/Assignment_2.html#airbnb-case-study",
    "href": "portfolio/patent-airbnb-analysis/Assignment_2.html#airbnb-case-study",
    "title": "Patent Success & Airbnb Demand Analysis",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe treat number of reviews as a stand-in for bookings and begin by exploring the 40,628-listing Airbnb-NYC dataset (features include listing age days, room type, bedrooms, bathrooms, nightly price, review scores for cleanliness / location / value, and an instant-bookable flag).\n\nHandling missing values – 76 listings lack bedrooms, 160 lack bathrooms, and about 10,200 lack all three review-score variables.\nMost of those 10 k are listings with zero reviews (9,481 rows, ≈ 23 % of the data).\nWe drop any row with a missing predictor to keep modeling simple, which chiefly removes those zero-review listings and leaves 30,160 listings (all with ≥ 1 review).\nShould we keep the zero-review rows?\nIncluding them would preserve information on brand-new hosts but requires imputing their absent review scores or using a two-part model.\nFor this tutorial we exclude them, accepting a bit of bias in exchange for cleaner predictors; we flag that trade-off for future work.\nFeature transformations –\ninstant_bookable is recoded from 't'/'f' to 0 / 1.\nNightly price is extremely right-skewed, so we model log_price instead, which stabilises variance and gives a roughly bell-shaped histogram.\ndays remains in raw units (median ≈ 3 years; one outlier appears at 117 years!), and no further transforms are applied at this stage.\n\nNext, let’s inspect the distribution of our key variables:\n\n\n\n\n\n\n\n\n\nLeft plot: Distribution of the number of reviews per listing (for listings with ≥1 review). The histogram is extremely right-skewed. A large fraction of listings have only a handful of reviews – for example, the median is 8 reviews, and 75% have ≤26 reviews. A long tail of popular listings have many more reviews (the maximum in this subset is 421). This heavy-tailed count distribution suggests that modeling approaches for count data (like Poisson regression) or a log-transformation may be appropriate. Note: ~23% of listings had 0 reviews (not shown here, as they were dropped for modeling), indicating many very new or less-booked listings.\nRight plot: Distribution of nightly price, in USD (left), and distribution of log-transformed price (right) for NYC Airbnb listings. The raw price distribution is highly skewed with most listings in the $50–$200 range and a few extreme outliers (up to $10,000). We limited the x-axis to $500 in the left plot for clarity, but even within this range the mass is concentrated at lower prices. The log-scale (natural log) of price, shown on the right, is much more symmetric and bell-shaped. This confirms that a log transformation of price will likely make modeling easier: a unit change in log_price corresponds to a multiplicative change in actual price, and we expect a more linear relationship with outcome variables on that scale.\nWith the data cleaned and initial insights gathered, we proceed to model the number of reviews (as a proxy for bookings) using two approaches: Poisson regression for count data, and linear regression. The response variable will be the count of reviews. In the linear model, we will use a log transformation of reviews to account for skewness, whereas the Poisson model will use the count directly with a log link function.\n\n\nPoisson Model\nA log link makes each coefficient a multiplicative bump.\n\\[\n\\ell(\\beta)=\\sum_i\\Bigl(y_iX_i’\\beta-\\exp(X_i’\\beta)-\\ln(y_i!)\\Bigr)\n\\]\n\n\n================================================================================================\n                                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                        3.0166      0.019    156.966      0.000       2.979       3.054\nC(room_type)[T.Private room]     0.0874      0.003     25.854      0.000       0.081       0.094\nC(room_type)[T.Shared room]     -0.1033      0.009    -11.345      0.000      -0.121      -0.085\ndays                          5.056e-05   3.93e-07    128.807      0.000    4.98e-05    5.13e-05\nbedrooms                         0.0464      0.002     22.733      0.000       0.042       0.050\nbathrooms                       -0.1453      0.004    -38.805      0.000      -0.153      -0.138\nlog_price                        0.1309      0.003     45.347      0.000       0.125       0.137\nreview_scores_cleanliness        0.1088      0.001     72.531      0.000       0.106       0.112\nreview_scores_location          -0.0975      0.002    -58.982      0.000      -0.101      -0.094\nreview_scores_value             -0.0794      0.002    -43.513      0.000      -0.083      -0.076\ninstant                          0.3521      0.003    121.730      0.000       0.346       0.358\n================================================================================================\n\nIncidence-rate ratios (IRR)\n Intercept                       20.42\nC(room_type)[T.Private room]     1.09\nC(room_type)[T.Shared room]      0.90\ndays                             1.00\nbedrooms                         1.05\nbathrooms                        0.86\nlog_price                        1.14\nreview_scores_cleanliness        1.11\nreview_scores_location           0.91\nreview_scores_value              0.92\ninstant                          1.42\ndtype: float64\n\n\nAfter fitting the Poisson model we learn, in plain English, that switching on Instant Book is the single biggest lever: it lifts the expected review count by roughly 42 percent. A one-point bump in the cleanliness score nudges bookings up by about 11 percent, while each additional year on the platform adds a modest 1 to 2 percent of extra reviews. Bigger homes help at the margin—more bedrooms bring slightly more traffic—whereas adding bathrooms on top of the existing bedroom count appears to signal a pricier, slower-turnover property and nudges counts down. Price itself shows a small positive elasticity once value is controlled, and the classic room-type hierarchy (private &gt; entire place &gt; shared) persists but only at the ten-percent edge. In short, the Poisson coefficients translate into a story where convenience (Instant Book), visible quality (cleanliness), and sensible capacity win the day, while sheer luxury features do not automatically drive higher volume.\n\n\nLinear regression on log reviews\nWe now fit a linear regression model using the same set of predictors, to compare results and illustrate trade-offs. A direct linear model on the count of reviews would violate linearity and normality assumptions (since the outcome is non-negative and very skewed). Therefore, we use \\(\\log(\\text{number\\_of\\_reviews})\\) as the response. This means we are modeling the (natural) log of review count, which should yield coefficients that can be interpreted somewhat like elasticities (percent changes). Note that since we dropped zero-review listings, \\(\\log(\\text{reviews})\\) is defined (for 1 review, log = 0). Had we included zeros, we would need to add a small constant (e.g. log(review+1)) or use Tobit models, but we avoided that issue by excluding zeros earlier.\n\n\n================================================================================================\n                                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                        1.6229      0.126     12.857      0.000       1.375       1.870\nC(room_type)[T.Private room]    -0.0015      0.021     -0.071      0.944      -0.044       0.041\nC(room_type)[T.Shared room]     -0.0083      0.053     -0.157      0.875      -0.112       0.095\ndays                             0.0001   6.36e-06     18.050      0.000       0.000       0.000\nbedrooms                         0.0509      0.013      3.850      0.000       0.025       0.077\nbathrooms                       -0.1159      0.023     -5.069      0.000      -0.161      -0.071\nlog_price                        0.1509      0.019      8.158      0.000       0.115       0.187\nreview_scores_cleanliness        0.1364      0.009     15.003      0.000       0.119       0.154\nreview_scores_location          -0.1062      0.011     -9.629      0.000      -0.128      -0.085\nreview_scores_value             -0.0616      0.012     -5.141      0.000      -0.085      -0.038\ninstant                          0.3823      0.020     18.990      0.000       0.343       0.422\n================================================================================================\n\n\nThe linear model summary indicates an \\(R^2 = 0.036\\) (3.6%), meaning the predictors explain only a few percent of the variance in log-reviews. This is extremely low, highlighting that there is a lot of unexplained variability (no surprise given how many idiosyncratic factors affect a listing’s popularity). By contrast, the Poisson’s pseudo-\\(R^2\\) was much higher, but note that pseudo-\\(R^2\\) is not directly comparable to OLS \\(R^2\\) – they measure different things (deviance vs variance explained).\nQuick lift chart – turning coefficients into dollars\n\n\n\n\n\n\n\n\n\nAverage lift = 8.26 reviews over the period\n\n\nMost hosts could expect ~6–7 extra reviews (about +40 %) by flipping Instant Book on—substantial given the median listing only has 8.\n\n\nConclusion\nPutting everything together, hosts who activate Instant Book, keep their place immaculately clean, and offer a sensibly-sized listing at a price guests deem fair can expect materially more bookings—on the order of six to seven extra reviews (≈ 40 %) over the period analysed. Room-type differences are secondary, and charging a premium does not hurt as long as guests still feel the value is there. Because we removed zero-review rows, these insights apply to listings that have at least begun to attract guests; a full funnel analysis would model the “first-review” hurdle separately. Nonetheless, both the Poisson and log-linear models agree on the headline levers, giving us confidence that cleanliness and instant-booking convenience matter far more than whether the sofa faces north or the bath towels are monogrammed."
  },
  {
    "objectID": "portfolio/fundraising-campaign-analysis/Assignment_1.html",
    "href": "portfolio/fundraising-campaign-analysis/Assignment_1.html",
    "title": "Fundraising Campaign Effectiveness Analysis",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn their study, Karlan and List discovered that announcing a matching grant significantly boosted both the probability and size of contributions, confirming that even simple price incentives can nudge donors into action. Intriguingly, however, larger match ratios (such as a $3:$1 match) did not outperform smaller ones (like $1:$1), suggesting that bigger “discounts” on giving may not always translate to bigger impacts. They also found the local political environment influenced donor responsiveness: individuals in conservative “red” states were more swayed by the matching offer than those in liberal “blue” states. This highlights that factors like community norms and trust can be just as critical as the financial structure of a fundraising campaign.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "portfolio/fundraising-campaign-analysis/Assignment_1.html#introduction",
    "href": "portfolio/fundraising-campaign-analysis/Assignment_1.html#introduction",
    "title": "Fundraising Campaign Effectiveness Analysis",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn their study, Karlan and List discovered that announcing a matching grant significantly boosted both the probability and size of contributions, confirming that even simple price incentives can nudge donors into action. Intriguingly, however, larger match ratios (such as a $3:$1 match) did not outperform smaller ones (like $1:$1), suggesting that bigger “discounts” on giving may not always translate to bigger impacts. They also found the local political environment influenced donor responsiveness: individuals in conservative “red” states were more swayed by the matching offer than those in liberal “blue” states. This highlights that factors like community norms and trust can be just as critical as the financial structure of a fundraising campaign.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "portfolio/fundraising-campaign-analysis/Assignment_1.html#data",
    "href": "portfolio/fundraising-campaign-analysis/Assignment_1.html#data",
    "title": "Fundraising Campaign Effectiveness Analysis",
    "section": "Data",
    "text": "Data\n\nDescription\nTo replicate Karlan and List’s findings, I first loaded their dataset and generated preliminary plots to get a feel for its contents. After displaying the first few rows to confirm the structure, I computed donation rates by treatment group and then prepared side-by-side visuals—bar plots for participation and histograms of donation amounts—to highlight the core outcome measures. These initial checks ensure that the data aligns with the original study’s composition before we proceed with more in-depth statistical testing and analysis.\nDetailed explanation of all the variables captured\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nPreview of results from study\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\nBar plot – Proportion Who Donated by Treatment Group\n\n\n\n\n\n\n\n\n\nIn this bar plot, each bar shows the proportion of people in the treatment or control group who made a donation, illustrating the immediate difference in participation rates.\nHistogram – Donation Amounts (Among Donors Only)\n\n\n\n\n\n\n\n\n\nHere, the histogram reveals the distribution of how much donors gave, helping us detect outliers, skewness, or other patterns in giving behavior.\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nAs part of the balance test, I conducted a hand-computed two-sample t-test to determine whether the treatment and control groups differed significantly on the variable mrm2 (months since last donation). The results showed that the treatment group had a mean of 13.012 months, while the control group had a mean of 12.998 months. The calculated t-statistic was 0.120 with an associated p-value of 0.9049. Since the p-value is well above the 0.05 threshold, we fail to reject the null hypothesis and conclude that there is no statistically significant difference between the groups. This result supports the validity of the randomization mechanism, as it suggests that both groups were balanced on this pre-treatment variable.\n\n\nHand-Composed Two-Sample t-Test for mrm2\n=========================================\nTreatment Mean (mrm2): 13.012, n=33395\nControl Mean (mrm2):   12.998, n=16687\nt-statistic:           0.120\nDegrees of freedom:    33394.48\np-value (two-sided):   0.9049\n\n\nTo validate these results using a different method, I also ran a simple linear regression where mrm2 was regressed on the treatment variable. This approach tests the same hypothesis as the t-test—that the average months since last donation is equal across groups. The estimated treatment effect (0.014) is nearly identical to the difference in group means, and the associated p-value (0.905) confirms the same conclusion: there is no statistically significant difference between the groups. This reinforces the finding that the randomization successfully created balanced groups.\n\n\nLinear regression (OLS)\nData                 : karlan_list_2007_pretty\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\nAs part of the balance test, I conducted a hand-computed two-sample t-test to determine whether the treatment and control groups differed significantly on the variable freq (number of prior donations). This represents the second variable tested for robustness, following the initial test on mrm2 (months since last donation). The results showed that the treatment group had a mean of 8.035 donations, while the control group had a mean of 8.047 donations. The calculated t-statistic was -0.111 with an associated p-value of 0.9117. Since the p-value is well above the 0.05 threshold, we fail to reject the null hypothesis and conclude that there is no statistically significant difference between the groups. The fact that both mrm2 and freq are balanced across treatment and control groups further validates the randomization mechanism, confirming that the two groups are comparable on key pre-treatment characteristics.\n\n\nHand-Composed Two-Sample t-Test for freq\n=========================================\nTreatment Mean (freq): 8.035, n=33396\nControl Mean (freq):   8.047, n=16687\nt-statistic:           -0.111\nDegrees of freedom:    33326.35\np-value (two-sided):   0.9117\n\n\nTo validate the results from the t-test, I also performed a linear regression of freq on the treatment variable. Like the t-test, this regression assesses whether there is a statistically significant difference in the number of prior donations between treatment and control groups. The coefficient on treatment (-0.012) closely matches the difference in group means, and the p-value (0.912) confirms the same conclusion: no significant difference exists. This consistency between the regression and the hand-calculated t-test reinforces the finding that the treatment assignment did not systematically influence pre-treatment donation frequency.\n\n\nLinear regression (OLS)\nData                 : karlan_list_2007_pretty\nResponse variable    : freq\nExplanatory variables: treatment\nNull hyp.: the effect of x on freq is zero\nAlt. hyp.: the effect of x on freq is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        8.047      0.088   91.231  &lt; .001  ***\ntreatment       -0.012      0.108   -0.111   0.912     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.012 df(1, 50081), p.value 0.912\nNr obs: 50,083\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese results conclude the Balance Test section and provide strong support for the success of the randomization mechanism."
  },
  {
    "objectID": "portfolio/fundraising-campaign-analysis/Assignment_1.html#experimental-results",
    "href": "portfolio/fundraising-campaign-analysis/Assignment_1.html#experimental-results",
    "title": "Fundraising Campaign Effectiveness Analysis",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nHere’s a short paragraph you can use to replace the todo sentence in the screenshot:\nThe bar plot below shows the proportion of individuals who made a donation in the treatment and control groups. This visualization offers an early look at the potential impact of being assigned to the treatment group, with a slightly higher donation rate observed. While this plot conveys similar information to the one presented in the Data section, it is revisited here in the context of hypothesis testing to begin assessing whether the observed difference is statistically significant.\n\n\n\n\n\n\n\n\n\nBased on the code used in our earlier balance checks, I ran a t‐test comparing the proportion who donated (gave == 1) across the treatment and control groups. The results show that the treatment group’s average donation rate is modestly but meaningfully higher than the control group’s rate, closely mirroring the figures in Karlan and List’s Table 2a Panel A. The p‐value from this test is well below the usual 5 percent threshold, indicating that the difference is unlikely to be by chance. In real terms, such a small bump in donation rates can significantly boost total contributions, demonstrating that even a simple intervention like a matching grant can alter donor behavior. This supports the broader conclusion that small “price” or matching signals can nudge people to act, thereby increasing charitable giving.\nHere’s a clear paragraph you can use to summarize the probit regression results in context with Table 3, Column 1 from the Karlan and List (2007) paper:\nI ran a probit regression to estimate whether being assigned to the treatment group significantly increased the likelihood of making a charitable donation. The model used gave as the binary outcome and treatment as the explanatory variable. The results show a positive and statistically significant coefficient of 0.0868 (p = 0.002), indicating that assignment to the treatment group is associated with a higher probability of donating. However, this estimate does not match the coefficient reported in Table 3, Column 1 of the original paper, which shows a much smaller effect size of 0.004 with a standard error of 0.001. Despite the model setup appearing consistent, the difference in results suggests there may be differences in the underlying implementation or sample filtering. Still, the significance of the treatment variable aligns with the paper’s broader conclusion: matching grants, even modest ones, can meaningfully shift donation behavior.\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nFri, 02 Jan 2026\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n21:07:31\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nTo evaluate the impact of different match ratios on donation behavior, I conducted both t-tests and a linear regression using dummy variables for the 1:1 (ratio1), 2:1 (ratio2), and 3:1 (ratio3) match groups. The t-tests showed that while donation rates were slightly higher in the 2:1 and 3:1 groups compared to 1:1, none of these differences were statistically significant. Donation probabilities ranged narrowly between 2.07% and 2.27%, suggesting that although the presence of a match increased giving, raising the match ratio did not lead to meaningful increases in participation.\nThe regression analysis confirmed these findings. Relative to the omitted baseline group, all three match treatments were associated with higher donation probabilities: +0.3 percentage points for 1:1 (p = 0.097), and +0.5 percentage points for both 2:1 and 3:1 (p-values = 0.006 and 0.005, respectively). However, the similarity in effect sizes between the 2:1 and 3:1 groups reinforces the idea of diminishing returns from more generous matches. Together, these results support the conclusion of Karlan and List (2007): while matching grants are effective overall, increasing the match ratio beyond 1:1 does not significantly boost donor participation.\n\n\n\n=== Checking ratio 2:1 vs. ratio 1:1 ===\nObservations in 2:1: 11134\nObservations in 1:1: 11133\nMean(gave) for 2:1: 0.0226\nMean(gave) for 1:1: 0.0207\nT-statistic: 0.9650  |  P-value: 0.3345\n\n=== Checking ratio 3:1 vs. ratio 1:1 ===\nObservations in 3:1: 11129\nObservations in 1:1: 11133\nMean(gave) for 3:1: 0.0227\nMean(gave) for 1:1: 0.0207\nT-statistic: 1.0150  |  P-value: 0.3101\n\n=== Checking ratio 2:1 vs. ratio 3:1 ===\nObservations in 2:1: 11134\nObservations in 3:1: 11129\nMean(gave) for 2:1: 0.0226\nMean(gave) for 3:1: 0.0227\nT-statistic: -0.0501  |  P-value: 0.96\n\n\nTo assess the effect of different match ratios on donation likelihood, I ran an OLS regression of the binary outcome gave on dummy variables ratio1, ratio2, and ratio3, representing 1:1, 2:1, and 3:1 match offers, respectively. The regression results show that all three match ratios are associated with higher donation rates compared to the omitted group, with estimated effects of 0.003 (p = 0.097) for 1:1, 0.005 (p = 0.006) for 2:1, and 0.005 (p = 0.005) for 3:1. The 2:1 and 3:1 coefficients are statistically significant at the 1% level, while 1:1 is only marginally significant at the 10% level. Despite these coefficients being small in magnitude (increasing likelihood of donation by 0.3 to 0.5 percentage points), they are meaningful in the context of mass fundraising. However, the similarity in effect size between 2:1 and 3:1 suggests diminishing returns to more generous matching—larger match ratios do not yield proportionally greater increases in donation rates. This conclusion is reinforced by direct calculation: the response rate difference between 1:1 and 2:1 is approximately 0.002, and between 2:1 and 3:1 is nearly zero (–0.0001), which mirrors the difference in regression coefficients. These findings align with the original study’s conclusion that while the presence of a match increases donations, increasing the match ratio beyond 1:1 offers no additional boost in donor participation.\n\n\nLinear regression (OLS)\nData                 : karlan_list_2007_pretty\nResponse variable    : gave\nExplanatory variables: ratio1, ratio2, ratio3\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio1           0.003      0.002    1.661   0.097    .\nratio2           0.005      0.002    2.744   0.006   **\nratio3           0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo assess the effectiveness of different matched donation sizes, I calculated the response rate differences directly from the data and compared those with differences from the regression coefficients. Direct computation from the data shows that moving from a 1:1 to a 2:1 match ratio increases the donation response rate by approximately 0.0019 (0.19 percentage points), while moving from 2:1 to 3:1 provides virtually no additional benefit (around 0.0001, or 0.01 percentage points).\nThe coefficients indicate that shifting from a 1:1 to a 2:1 match leads to an increase in donation likelihood by about 0.0019 (0.19 percentage points), which is statistically significant (p = 0.006). However, increasing the match ratio further, from 2:1 to 3:1, offers almost no additional improvement (just 0.0001), indicating diminishing returns for more generous match ratios. These results align closely with Karlan and List’s original conclusions, demonstrating that while the presence of a match is effective at boosting donation rates, making the match more generous beyond a certain point does not significantly increase donor participation.\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Fri, 02 Jan 2026   Prob (F-statistic):             0.0118\nTime:                        21:07:31   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Response Rate Differences (From Regression Coefficients) ===\nDifference (2:1 vs. 1:1): 0.0019\nDifference (3:1 vs. 2:1): 0.0001\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nFrom the results of the t-test comparing donation amounts between the treatment and control groups, we observe a mean donation of approximately $0.97 for the treatment group and $0.81 for the control group, indicating a moderate increase in donation amounts due to treatment. The calculated t-statistic is 1.9183, and the associated p-value is 0.05509, just above the conventional significance threshold of 0.05. This result suggests that while there is some evidence that the treatment leads to higher average donations, the difference is only marginally statistically significant. Thus, we learn that matching grants have the potential not only to increase participation but possibly also to increase donation amounts among all individuals (though evidence for this latter effect is weaker). Further investigation, potentially with larger sample sizes or more targeted treatments, would be beneficial to clarify this effect.\n\n\nT-test comparing donation amounts between treatment and control groups\nT-statistic: 1.9183\np-value: 0.05509\nMean donation (Treatment): 0.9669\nMean donation (Control): 0.8133\n\n\nThe regression results presented indicate that, among individuals who chose to donate (conditional donors), the estimated average donation for those in the control group is approximately $45.54 (intercept). The coefficient for the treatment variable is -1.668, suggesting that, on average, donors assigned to the treatment group donate about $1.67 less than those in the control group. However, this difference is not statistically significant, as indicated by the large p-value (0.561). Therefore, we cannot confidently conclude that the treatment had any substantial effect on the donation amount among people who donated.\nImportantly, the treatment coefficient here does have a causal interpretation because the original experimental design involves random assignment of individuals to treatment and control groups. Thus, the observed effect (or lack thereof) can reasonably be interpreted as causal. In this case, we learn that, conditional on donating, the matching treatment does not significantly influence the donation size—supporting the earlier observation that the main benefit of matching grants appears primarily in encouraging participation, rather than increasing amounts among existing donors.\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Fri, 02 Jan 2026   Prob (F-statistic):              0.561\nTime:                        21:07:31   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\ntodo: Make two plots: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "portfolio/fundraising-campaign-analysis/Assignment_1.html#simulation-experiment-law-of-large-numbers-central-limit-theorem",
    "href": "portfolio/fundraising-campaign-analysis/Assignment_1.html#simulation-experiment-law-of-large-numbers-central-limit-theorem",
    "title": "Fundraising Campaign Effectiveness Analysis",
    "section": "Simulation Experiment (Law of Large Numbers + Central Limit Theorem)",
    "text": "Simulation Experiment (Law of Large Numbers + Central Limit Theorem)\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem. Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p = 0.018 that a donation is made, and respondents who do get a charitable donation match of any size have a probability p = 0.022 of donating.\nThe first plot illustrates the Law of Large Numbers by showing the cumulative average difference between simulated treatment and control groups across 10,000 draws. Initially, the differences fluctuate widely due to small sample sizes, but as the number of observations grows, the cumulative average difference gradually stabilizes and converges near the true difference of 0.004.\nThe next four histograms demonstrate the Central Limit Theorem. Each histogram displays the distribution of 1000 repeated samples of average differences between the two groups for varying sample sizes (50, 200, 500, and 1000). For smaller samples (n=50), the distribution is wide, irregular, and centered near the true difference (0.004) but with considerable variation. As sample size increases (n=200, 500, and 1000), the distributions become progressively narrower and more symmetrical, clearly approximating a normal distribution and tightly centering around the true mean difference of 0.004. These results illustrate how increasing sample size improves precision, reduces variability, and provides the basis for robust statistical inference, as embodied by the t-statistic."
  },
  {
    "objectID": "portfolio/fundraising-campaign-analysis/Assignment_1.html#summary-conclusion",
    "href": "portfolio/fundraising-campaign-analysis/Assignment_1.html#summary-conclusion",
    "title": "Fundraising Campaign Effectiveness Analysis",
    "section": "Summary / Conclusion",
    "text": "Summary / Conclusion\nIn summary, this replication of Karlan and List’s (2007) study successfully validated their key findings. Matching grants indeed significantly increased the likelihood of donors contributing, confirming the power of simple financial incentives. Interestingly, while the presence of a match effectively boosted participation, increasing the match ratio beyond a basic 1:1 offer showed minimal additional impact, highlighting diminishing returns for more generous incentives. Further, the analysis indicated that the treatment primarily influences the decision to donate rather than the amount donated among those who chose to give. Lastly, simulation experiments effectively illustrated fundamental statistical principles: the Law of Large Numbers demonstrated how averages stabilize around true effects with larger samples, while the Central Limit Theorem showed how sampling distributions become increasingly normal and precise as sample sizes grow. Together, these analyses reinforce the importance of robust experimental design and statistical reasoning in evaluating charitable fundraising strategies."
  }
]