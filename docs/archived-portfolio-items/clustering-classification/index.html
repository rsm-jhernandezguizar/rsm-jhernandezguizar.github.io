<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.5">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Juan Hernández Guizar">
<meta name="dcterms.date" content="2025-06-11">
<meta name="description" content="K-means clustering and machine learning applications">

<title>Clustering &amp; Classification Methods – Juan Hernández Guizar</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ff4371ef257df69894857e99c6ad0d06.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-f735cf07a6910d811a1cbb5bbbcb55bb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Juan Hernández Guizar</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text"><i class="bi bi-house"></i> Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Resume.html"> 
<span class="menu-text"><i class="bi bi-newspaper"></i> Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../portfolio.html"> 
<span class="menu-text"><i class="bi bi-bar-chart"></i> Portfolio</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link active" data-scroll-target="#k-means-clustering">K-Means Clustering</a>
  <ul class="collapse">
  <li><a href="#implementing-k-means-from-scratch" id="toc-implementing-k-means-from-scratch" class="nav-link" data-scroll-target="#implementing-k-means-from-scratch">Implementing K-Means from Scratch</a></li>
  <li><a href="#visualizing-the-k-means-iterations" id="toc-visualizing-the-k-means-iterations" class="nav-link" data-scroll-target="#visualizing-the-k-means-iterations">Visualizing the K-Means Iterations</a></li>
  <li><a href="#comparing-with-scikit-learn" id="toc-comparing-with-scikit-learn" class="nav-link" data-scroll-target="#comparing-with-scikit-learn">Comparing with Scikit-Learn:</a></li>
  <li><a href="#choosing-the-optimal-number-of-clusters" id="toc-choosing-the-optimal-number-of-clusters" class="nav-link" data-scroll-target="#choosing-the-optimal-number-of-clusters">Choosing the Optimal Number of Clusters</a></li>
  </ul></li>
  <li><a href="#k-nearest-neighbors-knn" id="toc-k-nearest-neighbors-knn" class="nav-link" data-scroll-target="#k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</a>
  <ul class="collapse">
  <li><a href="#synthetic-dataset-with-a-nonlinear-boundary" id="toc-synthetic-dataset-with-a-nonlinear-boundary" class="nav-link" data-scroll-target="#synthetic-dataset-with-a-nonlinear-boundary">Synthetic Dataset with a Nonlinear Boundary</a></li>
  <li><a href="#implementing-knn-from-scratch" id="toc-implementing-knn-from-scratch" class="nav-link" data-scroll-target="#implementing-knn-from-scratch">Implementing KNN from Scratch</a></li>
  <li><a href="#finding-the-best-k-accuracy-vs.-k" id="toc-finding-the-best-k-accuracy-vs.-k" class="nav-link" data-scroll-target="#finding-the-best-k-accuracy-vs.-k">Finding the Best k (Accuracy vs.&nbsp;k)</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Clustering &amp; Classification Methods</h1>
</div>

<div>
  <div class="description">
    K-means clustering and machine learning applications
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Juan Hernández Guizar </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 11, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="k-means-clustering" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering">K-Means Clustering</h2>
<p>K-Means is an <strong>unsupervised</strong> learning algorithm that groups unlabeled data into k clusters based on similarity. The goal is to partition the data so that points in the same cluster are more similar to each other than to those in other clusters ￼. In essence, K-Means tries to find cluster centers (called centroids) that minimize the distance of each point to its nearest centroid ￼.</p>
<p>How does K-Means work? At a high level, the algorithm follows an iterative refinement procedure:</p>
<ul>
<li><strong>Initialize</strong> – Choose k initial centroids (often random picks from the data).</li>
<li><strong>Assign</strong> – For each point, find the nearest centroid (by Euclidean distance) and assign the point to that cluster.</li>
<li><strong>Update</strong> – Recompute each centroid as the average (mean) of all points assigned to it.</li>
<li><strong>Repeat</strong> – Iterate the assign-update steps until centroids stop changing (convergence).</li>
</ul>
<p>This process will partition the dataset into k clusters such that each point belongs to the cluster with the closest centroid. The algorithm stops when successive iterations no longer change the centroids (or change them negligibly), meaning the clustering has stabilized. The result is a set of clusters and their centroid locations.</p>
<p>To demonstrate K-Means, we’ll use the Palmer Penguins dataset, a popular alternative to the iris dataset. It contains measurements for three penguin species (Adelie, Chinstrap, Gentoo) from islands in Antarctica. We will use just two features for clustering: bill length and flipper length (both in mm). This gives us a 2D dataset that we can easily visualize. We will ignore the species labels during clustering (since K-Means is unsupervised), but it’s worth noting there are 3 true species in the data (which might correspond to 3 clusters).</p>
<p>First, let’s load the dataset and take a peek at the data structure:</p>
<div id="9d7d30c0" class="cell" data-execution_count="1">
<div class="cell-output cell-output-stdout">
<pre><code>  species  bill_length_mm  flipper_length_mm
0  Adelie            39.1                181
1  Adelie            39.5                186
2  Adelie            40.3                195
3  Adelie            36.7                193
4  Adelie            39.3                190
Dataset shape: (333, 2)</code></pre>
</div>
</div>
<p>We have 332 penguin observations with bill length and flipper length. Now, let’s implement the K-Means algorithm from scratch for a chosen number of clusters, K=3. (Choosing 3 is a reasonable guess here given the three species, but we will later analyze different k values.)</p>
<section id="implementing-k-means-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="implementing-k-means-from-scratch">Implementing K-Means from Scratch</h3>
<p>We’ll write a simple implementation of K-Means. The plan:</p>
<ol type="1">
<li><p>Randomly initialize 3 centroids by selecting 3 random points from the dataset.</p></li>
<li><p>Loop until convergence:</p></li>
</ol>
<ul>
<li>Compute the distance from each data point to each centroid.</li>
<li>Assign each point to the nearest centroid (forming 3 clusters).</li>
<li>Recompute each centroid as the mean of the points in its cluster.</li>
<li>If centroids don’t change (or change very little), break out.</li>
</ul>
<p>We’ll also keep track of the cluster assignments at each iteration so we can visualize the progression.</p>
<div id="968d1c78" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data as a numpy array for convenience</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins.to_numpy()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># K-Means parameters</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly choose K unique indices for initial centroids</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>initial_idx <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X), K, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>centroids <span class="op">=</span> X[initial_idx]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initial centroids (randomly chosen):</span><span class="ch">\n</span><span class="st">"</span>, centroids)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># K-Means iterative process</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>max_iters <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>centroid_history <span class="op">=</span> [centroids.copy()]  <span class="co"># store centroids at each iteration</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>cluster_history <span class="op">=</span> []  <span class="co"># store cluster assignments at each iteration</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> itr <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Assign points to the nearest centroid</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    distances <span class="op">=</span> np.linalg.norm(</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        X[:, <span class="va">None</span>] <span class="op">-</span> centroids[<span class="va">None</span>, :], axis<span class="op">=</span><span class="dv">2</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># distance to each centroid</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    clusters <span class="op">=</span> np.argmin(distances, axis<span class="op">=</span><span class="dv">1</span>)  <span class="co"># index of nearest centroid for each point</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    cluster_history.append(clusters)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Update centroids to the mean of assigned points</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    new_centroids <span class="op">=</span> np.array(</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            X[clusters <span class="op">==</span> k].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">if</span> np.<span class="bu">any</span>(clusters <span class="op">==</span> k) <span class="cf">else</span> centroids[k]</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check for convergence (if centroids are unchanged)</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.allclose(new_centroids, centroids):</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> new_centroids</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        centroid_history.append(centroids.copy())</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Converged after </span><span class="sc">{</span>itr<span class="sc">}</span><span class="ss"> iterations."</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> new_centroids</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    centroid_history.append(centroids.copy())</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Final centroids and cluster assignment</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>final_centroids <span class="op">=</span> centroids</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>final_clusters <span class="op">=</span> cluster_history[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final centroids:</span><span class="ch">\n</span><span class="st">"</span>, final_centroids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initial centroids (randomly chosen):
 [[ 39.5 178. ]
 [ 50.9 196. ]
 [ 42.1 195. ]]
Converged after 9 iterations.
Final centroids:
 [[ 38.45304348 187.05217391]
 [ 47.6296     216.92      ]
 [ 45.95483871 196.7311828 ]]</code></pre>
</div>
</div>
<p>Next, we’ll visualize the clustering process to see how K-Means reached this result.</p>
</section>
<section id="visualizing-the-k-means-iterations" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-k-means-iterations">Visualizing the K-Means Iterations</h3>
<p>To better understand K-Means, it helps to visualize how the centroids move and how points switch clusters over iterations. We will plot the data points colored by their cluster at each iteration, and show the centroid positions. An animated GIF can illustrate the process over time. Below, we generate plots for each iteration and combine them into a GIF:</p>
<p><img src="kmeans_steps.gif" class="img-fluid" alt="Animated centroid updates" width="600"></p>
<p>Now we have an animation showing the algorithm’s progress. In the first frame, the centroids start at random positions. With each iteration, points get re-assigned (colors may change), and centroids move towards the center of their new clusters (arrows show the movement). The process continues until the movements are negligible.</p>
<p>K-Means clustering on the penguins data – after the first iteration. Arrows indicate how the centroids (black X markers) moved from their initial random positions to new positions (black circles) after re-computing the means for each cluster.</p>
<p>Final cluster assignment for K=3 on the Palmer Penguins data (bill length vs flipper length). Each color represents a cluster found by our algorithm, and black X’s are the final centroid positions.</p>
<p>In the final clustering above, we see three distinct clusters of penguins. It turns out these clusters largely correspond to the three species (Adelie, Chinstrap, Gentoo), even though we did not use the species labels during clustering. Our K-Means algorithm essentially “discovered” groupings similar to the actual species by just using the two features.</p>
</section>
<section id="comparing-with-scikit-learn" class="level3">
<h3 class="anchored" data-anchor-id="comparing-with-scikit-learn">Comparing with Scikit-Learn:</h3>
<p>To ensure our implementation is correct, we can compare with scikit-learn’s built-in K-Means. Scikit-learn uses the same objective (minimizing sum of squared distances) and by default uses the K-Means++ initialization for centroids (a smart way to pick initial centroids). We fit scikit’s model on the same data:</p>
<div id="163323cb" class="cell" data-execution_count="4">
<div class="cell-output cell-output-stdout">
<pre><code>Sklearn final centroids:
 [[ 45.94574468 196.84042553]
 [ 47.65       217.        ]
 [ 38.45304348 187.05217391]]</code></pre>
</div>
</div>
<p>If we check, the centroids from scikit-learn are very close to those from our implementation, and the cluster assignments are effectively the same (just label order might differ). This gives us confidence that our scratch implementation worked correctly.</p>
</section>
<section id="choosing-the-optimal-number-of-clusters" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-optimal-number-of-clusters">Choosing the Optimal Number of Clusters</h3>
<p>In practice, we usually don’t know the best value of K upfront. Choosing K is part of the challenge in clustering. Two common methods to evaluate different K values are:</p>
<ul>
<li><p><strong>Within-Cluster Sum of Squares (WCSS):</strong> This is the sum of squared distances from each point to its cluster centroid (also known as cluster “inertia”). Lower WCSS means clusters are tighter. As K increases, WCSS always decreases (more clusters reduce within-cluster variance). We can plot WCSS for K=2,3,… and look for an “elbow point” – where the rate of improvement slows. This is the Elbow Method.</p></li>
<li><p><strong>Silhouette Score:</strong> This measures how well-separated the clusters are. Silhouette score for a point is defined as (b - a) / max(a, b), where a is the average distance to other points in the same cluster (cohesion) and b is the average distance to points in the nearest other cluster (separation) ￼. The score ranges from -1 to 1; higher means the point is in the right cluster (well separated from others) ￼. We often use the average silhouette over all points to evaluate the clustering quality for a given K. A higher average silhouette indicates more distinct clustering structure.</p></li>
</ul>
<p>Let’s compute these metrics for K = 2 through 7 and see which K might be best for our penguin data:</p>
<div id="01d55370" class="cell" data-execution_count="5">
<div class="cell-output cell-output-stdout">
<pre><code>WCSS for K=2..7: [20949.785311278196, 13859.543763777467, 9587.135276652696, 7424.8815080702425, 6332.786502814991, 5257.088985919958]
Silhouette scores for K=2..7: [0.6117940477662409, 0.48062618684371067, 0.4448839684032104, 0.42719392387268307, 0.41278625329909713, 0.4307326685776095]</code></pre>
</div>
</div>
<p>Plotting these values:</p>
<div id="68c46d5e" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-7-output-1.png" width="949" height="372" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Evaluating different cluster counts on the penguin data. Left: WCSS (sum of squared distances within clusters) for K=2 to 7. Right: Average silhouette score for K=2 to 7. Higher silhouette is better.</p>
<p>From the above, WCSS steadily decreases as K increases (as expected), and there’s a bit of a knee around K=3-4 (the “elbow” is not super sharp here). The silhouette score peaks at K=2 (about 0.61) and then drops for higher K, hitting a low around 0.41–0.44 for K=4-6, with a slight uptick at K=7. A high silhouette means clusters are well separated, so this suggests that using 2 clusters yields the clearest separation in this 2D feature space.</p>
<p>This is interesting because we know there are 3 species. What’s happening is that two of the species (Adelie and Chinstrap) have very similar bill/flipper measurements that overlap a lot, so the algorithm doesn’t find a clear separation between them and favors combining them into one cluster. The Gentoo penguins, on the other hand, form a very distinct cluster. Thus, K=2 gives two well-separated clusters (basically “Gentoo” and “Adelie+Chinstrap”), which yields a higher silhouette score than K=3 where the algorithm is forced to split the Adelie/Chinstrap group and ends up with one weaker separation. In a real analysis, we’d consider this trade-off: do we want 3 clusters to match known species, or 2 clusters that are most distinct in these features? There’s no single correct answer – it depends on the goal. But this illustrates how WCSS and silhouette can guide the choice of K.</p>
</section>
</section>
<section id="k-nearest-neighbors-knn" class="level2">
<h2 class="anchored" data-anchor-id="k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</h2>
<p>Next, let’s switch gears to a <strong>supervised</strong> learning method: K-Nearest Neighbors. KNN is one of the simplest classification algorithms. The basic idea: to predict the class of a new point, look at the “k” closest points in the training data and take a majority vote. In other words, the label is determined by the plurality of the labels of its k nearest neighbors in the training set. If k=5, the 5 closest neighbors’ labels are used – if 3 of them are class A and 2 are class B, the new point is classified as A (majority wins). KNN makes no assumptions about data distribution (it’s a non-parametric, instance-based method), and it’s considered a “lazy” learner because it doesn’t build an explicit model; it just stores the training instances and defers computation to query time.</p>
<p>Choosing k: The parameter k controls the model’s complexity. A small k (e.g.&nbsp;1) means the classifier can be very flexible and even noisy – essentially memorizing the training data (low bias, high variance). A large k means we smooth over more neighbors, making the classifier more stable but potentially less sensitive to local patterns (high bias, low variance). We’ll see this trade-off in action with a synthetic example.</p>
<section id="synthetic-dataset-with-a-nonlinear-boundary" class="level3">
<h3 class="anchored" data-anchor-id="synthetic-dataset-with-a-nonlinear-boundary">Synthetic Dataset with a Nonlinear Boundary</h3>
<p>To illustrate KNN, we’ll create a 2D synthetic dataset for binary classification. We want something non-linear to show KNN’s strength at capturing complex boundaries. We generate <strong>100</strong> random points uniformly inside the square <span class="math display">\[
(x_1,x_2)\in[-3,3]
\]</span></p>
<p>The true boundary is the wiggly curve</p>
<p><span class="math display">\[
x_2 \;=\; \sin(4x_1)+x_1,
\]</span></p>
<p>so we label a point <strong>class 1</strong> when <span class="math inline">\(x_2\)</span> lies <strong>above</strong> the curve and <strong>class 0</strong> otherwise. This produces two interwoven regions separated by a sine wave, giving KNN a genuinely non‑linear problem.</p>
<div id="3ade7b35" class="cell" data-execution_count="7">
<div class="cell-output cell-output-stdout">
<pre><code>Training set class counts: [49 51]
Test set class counts: [53 47]</code></pre>
</div>
</div>
<p>We have created a training set of 100 points and a test set of 100 points. Let’s visualize the training data and the true boundary:</p>
<div id="95df7c15" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="540" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>*Synthetic dataset: 100 training points (blue = class 0, red = class 1) in the square <span class="math inline">\([-3,3]^2\)</span></p>
<p>The green dashed line is the true boundary</p>
<p><span class="math display">\[
x_2 \;=\; \sin(4x_1)+x_1,
\]</span></p>
<p>You can see that class 0 (blue) occupies the area below the sine curve, and class 1 (red) is above the curve. The boundary wiggles up and down. No linear model could classify this perfectly, but KNN should do well given enough neighbors.</p>
</section>
<section id="implementing-knn-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="implementing-knn-from-scratch">Implementing KNN from Scratch</h3>
<p>The KNN algorithm for classification is straightforward to implement. For each query (test point), we need to find the distances to all training points, pick the k nearest, and take a majority vote of their labels. A basic implementation might be O(n) per query (where n is number of training points), which can be slow for large datasets, but for our data size it’s fine.</p>
<p>We’ll write a function knn_predict(X_train, y_train, X_test, k) that returns predicted labels for the X_test points:</p>
<div id="24937c90" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> knn_predict(X_train, y_train, X_test, k):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> X_test:</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute distance from x to all points in X_train (Euclidean)</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        distances <span class="op">=</span> np.linalg.norm(X_train <span class="op">-</span> x, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find the indices of the k nearest neighbors</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        nn_idx <span class="op">=</span> np.argsort(distances)[:k]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        nn_labels <span class="op">=</span> y_train[nn_idx]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Majority vote: the predicted class is the mode of the neighbor labels</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For binary labels 0/1, we can just take the mean and round it</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        vote <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> nn_labels.mean() <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        predictions.append(vote)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(predictions)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Try out the KNN predictor for k=5 on a few test points</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>y_pred_test_k5 <span class="op">=</span> knn_predict(X_train, y_train, X_test[:<span class="dv">5</span>], k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"True labels:     "</span>, y_test[:<span class="dv">5</span>])</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted labels:"</span>, y_pred_test_k5)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True labels:      [1 1 0 1 1]
Predicted labels: [1 1 0 1 1]</code></pre>
</div>
</div>
<p>In practice, one can use spatial data structures (KD-trees, ball trees) for large datasets, but we’re keeping it simple. The majority vote is implemented by averaging the 0/1 labels and rounding – this works because if a majority are 1s, the average &gt; 0.5, otherwise it’s &lt; 0.5 (for multiclass, you’d do something like collections.Counter or np.bincount to get the mode).</p>
<p>Let’s see how our scratch KNN performs and verify it against scikit-learn’s KNN classifier:</p>
<div id="9cf19c9e" class="cell" data-execution_count="10">
<div class="cell-output cell-output-stdout">
<pre><code>Our KNN vs sklearn KNN match: True
Test accuracy (k=5) scratch: 0.900
Test accuracy (k=5) sklearn: 0.900</code></pre>
</div>
</div>
<p>If we run this, we find that our implementation’s predictions exactly match scikit-learn’s for k=5 (and indeed they should for any k). The accuracy on the test set for k=5 is around 0.9 (90%). It’s good to confirm that our scratch code is working properly.</p>
</section>
<section id="finding-the-best-k-accuracy-vs.-k" class="level3">
<h3 class="anchored" data-anchor-id="finding-the-best-k-accuracy-vs.-k">Finding the Best k (Accuracy vs.&nbsp;k)</h3>
<p>Finally, let’s see how the choice of k affects the performance on this problem. We’ll evaluate k from 1 up to 30 and record the classification accuracy on the test set for each:</p>
<div id="d10f57eb" class="cell" data-execution_count="11">
<div class="cell-output cell-output-stdout">
<pre><code>Best k = 1 with test accuracy = 0.920</code></pre>
</div>
</div>
<p>Plotting these accuracies as a function of k:</p>
<div id="89694cd0" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-13-output-1.png" width="523" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The exact numbers may vary slightly with random seed, but the trend is clear: the highest accuracy in this case is about 92 % for the best k, then it drops off as k increases. The plot of accuracy vs k would show a peak at a very low k, then a gradual decline.</p>
<p>Why does this happen? Our synthetic data has a very clean, deterministic boundary (no noise in labels given the features). A very flexible model like 1-nearest-neighbor can trace that wavy boundary almost perfectly – in fact, 1-NN yields about 92 % accuracy on the test! It slightly overfits to the discrete sample (if we had infinite data, 1-NN would eventually perfectly model the true boundary). Using a larger k smooths the decision boundary. For instance, at k=5 we saw accuracy around 0.9 (90%) – some fine detail is lost, as the model sometimes errs on points near the boundary by averaging in neighbors from the other side of the wave. By k=30, accuracy drops further, because with such a large neighborhood, the classifier is getting too biased – it’s oversmoothing and doesn’t capture the wiggles at all (in the extreme, if you used k equal to the entire training set, it would always predict the majority class, ignoring the input features altogether!). This illustrates the bias-variance trade-off in KNN: small k = low bias, high variance; large k = high bias, low variance. The optimal k is often somewhere in between, and one usually uses cross-validation to find it in practice. In our case, since the data has no label noise, the best performance is at the most flexible end (k=1 or 3).</p>
<p>To visualize what KNN is doing, here’s a depiction of the decision boundary learned by our model for two different k values: - For k=1, the decision boundary will essentially trace around every red point (each training point defines its own region via a Voronoi partition). It will be very jagged and pass through narrow gaps – perfectly fitting the training data, but potentially too wiggly for noisy data. - For k=15 (for example), the decision boundary will be much smoother – small irregularities are averaged out, and only broader trends remain. It might misclassify some points that lie in small “bays” of the opposite class, as it favors the overall majority in a larger neighborhood ￼.</p>
<p>In our synthetic example, the boundary was truly sinusoidal, so a very flexible model wins. If we had added noise or outliers, a slightly larger k would likely be better to avoid chasing that noise. This is a great demonstration of how KNN’s parameter k influences the model complexity.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>We’ve created a step-by-step walkthrough of K-Means and K-Nearest Neighbors, implementing both from scratch and validating against library implementations. For K-Means, we clustered penguins by their physical measurements and used WCSS and silhouette analysis to reason about the appropriate number of clusters. For KNN, we built a synthetic classification problem with a non-linear boundary and saw how the choice of neighbors affects performance. Along the way, we visualized the clustering process (with an animated centroid update plot) and the classification boundary (implicitly, through the accuracy vs k behavior and discussion).</p>
<p>Key Takeaways:</p>
<ul>
<li><p><strong>K-Means</strong> is an unsupervised clustering algorithm that iteratively assigns points to the nearest centroid and updates centroids to the mean of points ￼. It aims to minimize within-cluster variance (WCSS). Choosing the number of clusters K is crucial – methods like the elbow plot and silhouette analysis help evaluate cluster quality.</p></li>
<li><p><strong>K-Nearest Neighbors</strong> is a simple yet powerful supervised learning method that classifies points based on the majority label of their nearest neighbors. It is easy to implement and makes no distribution assumptions (non-parametric). The parameter k controls the bias-variance trade-off: smaller k can capture fine-grained patterns but may overfit, while larger k smooths out noise but may miss details.</p></li>
<li><p><strong>Both algorithms are intuitive</strong> K-Means finds natural groupings in data, and KNN makes predictions by “consulting” nearby examples. Despite their simplicity, they are effective for a wide range of problems and are excellent for building intuition about clustering and classification.</p></li>
</ul>
<p>Hopefully, this walkthrough helped clarify how K-Means and KNN work. Feel free to experiment with different parameters or datasets – tweak the number of clusters, or try KNN on a different boundary shape – to further solidify your understanding. Happy learning!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>