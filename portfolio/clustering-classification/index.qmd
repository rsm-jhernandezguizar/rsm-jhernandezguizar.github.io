---
title: "Clustering & Classification Methods"
description: "K-means clustering and machine learning applications"
image: /images/assignment4.jpg
date: 2025-06-11
author: Juan Hernández Guizar
---

## K-Means Clustering

K-Means is an **unsupervised** learning algorithm that groups unlabeled data into k clusters based on similarity. The goal is to partition the data so that points in the same cluster are more similar to each other than to those in other clusters ￼. In essence, K-Means tries to find cluster centers (called centroids) that minimize the distance of each point to its nearest centroid ￼.

How does K-Means work? At a high level, the algorithm follows an iterative refinement procedure:

- **Initialize** – Choose k initial centroids (often random picks from the data).
- **Assign** – For each point, find the nearest centroid (by Euclidean distance) and assign the point to that cluster.
- **Update** – Recompute each centroid as the average (mean) of all points assigned to it.
- **Repeat** – Iterate the assign-update steps until centroids stop changing (convergence).

This process will partition the dataset into k clusters such that each point belongs to the cluster with the closest centroid. The algorithm stops when successive iterations no longer change the centroids (or change them negligibly), meaning the clustering has stabilized. The result is a set of clusters and their centroid locations.

To demonstrate K-Means, we’ll use the Palmer Penguins dataset, a popular alternative to the iris dataset. It contains measurements for three penguin species (Adelie, Chinstrap, Gentoo) from islands in Antarctica. We will use just two features for clustering: bill length and flipper length (both in mm). This gives us a 2D dataset that we can easily visualize. We will ignore the species labels during clustering (since K-Means is unsupervised), but it’s worth noting there are 3 true species in the data (which might correspond to 3 clusters).

First, let’s load the dataset and take a peek at the data structure:

```{python}
# | echo: false
import pandas as pd

# Load the Palmer Penguins dataset (CSV file provided)
df = pd.read_csv("data/palmer_penguins.csv")
print(df[["species", "bill_length_mm", "flipper_length_mm"]].head())

# Drop any rows with missing values in the features of interest
penguins = df[["bill_length_mm", "flipper_length_mm"]].dropna()
print("Dataset shape:", penguins.shape)
```

We have 332 penguin observations with bill length and flipper length. Now, let’s implement the K-Means algorithm from scratch for a chosen number of clusters, K=3. (Choosing 3 is a reasonable guess here given the three species, but we will later analyze different k values.)

### Implementing K-Means from Scratch

We’ll write a simple implementation of K-Means. The plan:

1.	Randomly initialize 3 centroids by selecting 3 random points from the dataset.

2.	Loop until convergence:

- Compute the distance from each data point to each centroid.
- Assign each point to the nearest centroid (forming 3 clusters).
- Recompute each centroid as the mean of the points in its cluster.
- If centroids don’t change (or change very little), break out.

We’ll also keep track of the cluster assignments at each iteration so we can visualize the progression.

```{python}
# | echo: true
import numpy as np

# Prepare data as a numpy array for convenience
X = penguins.to_numpy()

# K-Means parameters
K = 3
np.random.seed(42)
# Randomly choose K unique indices for initial centroids
initial_idx = np.random.choice(len(X), K, replace=False)
centroids = X[initial_idx]
print("Initial centroids (randomly chosen):\n", centroids)

# K-Means iterative process
max_iters = 100
centroid_history = [centroids.copy()]  # store centroids at each iteration
cluster_history = []  # store cluster assignments at each iteration

for itr in range(max_iters):
    # Step 1: Assign points to the nearest centroid
    distances = np.linalg.norm(
        X[:, None] - centroids[None, :], axis=2
    )  # distance to each centroid
    clusters = np.argmin(distances, axis=1)  # index of nearest centroid for each point
    cluster_history.append(clusters)

    # Step 2: Update centroids to the mean of assigned points
    new_centroids = np.array(
        [
            X[clusters == k].mean(axis=0) if np.any(clusters == k) else centroids[k]
            for k in range(K)
        ]
    )
    # Check for convergence (if centroids are unchanged)
    if np.allclose(new_centroids, centroids):
        centroids = new_centroids
        centroid_history.append(centroids.copy())
        print(f"Converged after {itr} iterations.")
        break
    centroids = new_centroids
    centroid_history.append(centroids.copy())

# Final centroids and cluster assignment
final_centroids = centroids
final_clusters = cluster_history[-1]
print("Final centroids:\n", final_centroids)
```

Next, we'll visualize the clustering process to see how K-Means reached this result.

### Visualizing the K-Means Iterations

To better understand K-Means, it helps to visualize how the centroids move and how points switch clusters over iterations. We will plot the data points colored by their cluster at each iteration, and show the centroid positions. An animated GIF can illustrate the process over time. Below, we generate plots for each iteration and combine them into a GIF:

```{python}
# | echo: false
import matplotlib.pyplot as plt
import imageio.v2 as imageio

frames = []
colors = ["#1f77b4", "#ff7f0e", "#2ca02c"]  # distinct colors for clusters 1,2,3
for i in range(1, len(centroid_history)):
    clusters = cluster_history[i - 1]
    old_centroids = centroid_history[i - 1]
    new_centroids = centroid_history[i]

    plt.figure(figsize=(6, 5))
    # Plot points, colored by cluster
    for k in range(K):
        pts = X[clusters == k]
        plt.scatter(
            pts[:, 0], pts[:, 1], c=colors[k], label=f"Cluster {k+1}", alpha=0.6
        )
    # Plot old and new centroids
    plt.scatter(
        old_centroids[:, 0],
        old_centroids[:, 1],
        marker="x",
        s=100,
        c="k",
        label="Old Centroid",
    )
    plt.scatter(
        new_centroids[:, 0],
        new_centroids[:, 1],
        marker="o",
        s=100,
        c="k",
        edgecolors="white",
        label="New Centroid",
    )
    # Draw arrows to show centroid movement
    for k in range(K):
        ox, oy = old_centroids[k]
        nx, ny = new_centroids[k]
        plt.arrow(
            ox,
            oy,
            nx - ox,
            ny - oy,
            color="gray",
            width=0.5,
            head_width=2.0,
            length_includes_head=True,
        )
    plt.title(f"K-Means Iteration {i}")
    plt.xlabel("Bill Length (mm)")
    plt.ylabel("Flipper Length (mm)")
    plt.legend(loc="upper right")
    plt.tight_layout()
    # Save frame
    filename = f"frame_{i}.png"
    plt.savefig(filename)
    plt.close()
    frames.append(imageio.imread(filename))

# Save frames as an animated GIF
# duration  = seconds PER frame   (larger  = slower)
# loop=0    = loop forever        (loop=1 would play once)
imageio.mimsave(
    "kmeans_steps.gif",
    frames,
    duration=10,
    loop=0,  # 0 = infinite looping
)
```

![](kmeans_steps.gif){fig-alt="Animated centroid updates" width=600}

Now we have an animation showing the algorithm’s progress. In the first frame, the centroids start at random positions. With each iteration, points get re-assigned (colors may change), and centroids move towards the center of their new clusters (arrows show the movement). The process continues until the movements are negligible.

K-Means clustering on the penguins data – after the first iteration. Arrows indicate how the centroids (black X markers) moved from their initial random positions to new positions (black circles) after re-computing the means for each cluster.

Final cluster assignment for K=3 on the Palmer Penguins data (bill length vs flipper length). Each color represents a cluster found by our algorithm, and black X’s are the final centroid positions.

In the final clustering above, we see three distinct clusters of penguins. It turns out these clusters largely correspond to the three species (Adelie, Chinstrap, Gentoo), even though we did not use the species labels during clustering. Our K-Means algorithm essentially “discovered” groupings similar to the actual species by just using the two features.

### Comparing with Scikit-Learn:
To ensure our implementation is correct, we can compare with scikit-learn’s built-in K-Means. Scikit-learn uses the same objective (minimizing sum of squared distances) and by default uses the K-Means++ initialization for centroids (a smart way to pick initial centroids). We fit scikit’s model on the same data:

```{python}
# | echo: false
from sklearn.cluster import KMeans

sk_km = KMeans(n_clusters=3, n_init=10, random_state=42)
sk_labels = sk_km.fit_predict(X)
print("Sklearn final centroids:\n", sk_km.cluster_centers_)
```

If we check, the centroids from scikit-learn are very close to those from our implementation, and the cluster assignments are effectively the same (just label order might differ). This gives us confidence that our scratch implementation worked correctly.

### Choosing the Optimal Number of Clusters

In practice, we usually don’t know the best value of K upfront. Choosing K is part of the challenge in clustering. Two common methods to evaluate different K values are:

- **Within-Cluster Sum of Squares (WCSS):** This is the sum of squared distances from each point to its cluster centroid (also known as cluster “inertia”). Lower WCSS means clusters are tighter. As K increases, WCSS always decreases (more clusters reduce within-cluster variance). We can plot WCSS for K=2,3,… and look for an “elbow point” – where the rate of improvement slows. This is the Elbow Method.

- **Silhouette Score:** This measures how well-separated the clusters are. Silhouette score for a point is defined as (b - a) / max(a, b), where a is the average distance to other points in the same cluster (cohesion) and b is the average distance to points in the nearest other cluster (separation) ￼. The score ranges from -1 to 1; higher means the point is in the right cluster (well separated from others) ￼. We often use the average silhouette over all points to evaluate the clustering quality for a given K. A higher average silhouette indicates more distinct clustering structure.

Let’s compute these metrics for K = 2 through 7 and see which K might be best for our penguin data:

```{python}
# | echo: false
from sklearn.metrics import silhouette_score

wcss_values = []
silhouette_values = []
for k in range(2, 8):
    km = KMeans(n_clusters=k, n_init=10, random_state=0)
    labels = km.fit_predict(X)
    wcss_values.append(km.inertia_)  # inertia_ is the WCSS (sum of squared distances)
    silhouette_values.append(silhouette_score(X, labels))

print("WCSS for K=2..7:", wcss_values)
print("Silhouette scores for K=2..7:", silhouette_values)
```

Plotting these values:

```{python}
# | echo: false
import matplotlib.pyplot as plt

Ks = range(2, 8)
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(Ks, wcss_values, marker="o")
plt.title("WCSS vs K (Elbow Method)")
plt.xlabel("Number of clusters K")
plt.ylabel("WCSS (Inertia)")

plt.subplot(1, 2, 2)
plt.plot(Ks, silhouette_values, marker="o", color="orange")
plt.title("Average Silhouette vs K")
plt.xlabel("Number of clusters K")
plt.ylabel("Silhouette Score")
plt.ylim(0, 1)
plt.tight_layout()
plt.show()
```

Evaluating different cluster counts on the penguin data. Left: WCSS (sum of squared distances within clusters) for K=2 to 7. Right: Average silhouette score for K=2 to 7. Higher silhouette is better.

From the above, WCSS steadily decreases as K increases (as expected), and there’s a bit of a knee around K=3-4 (the “elbow” is not super sharp here). The silhouette score peaks at K=2 (about 0.61) and then drops for higher K, hitting a low around 0.41–0.44 for K=4-6, with a slight uptick at K=7. A high silhouette means clusters are well separated, so this suggests that using 2 clusters yields the clearest separation in this 2D feature space.

This is interesting because we know there are 3 species. What’s happening is that two of the species (Adelie and Chinstrap) have very similar bill/flipper measurements that overlap a lot, so the algorithm doesn’t find a clear separation between them and favors combining them into one cluster. The Gentoo penguins, on the other hand, form a very distinct cluster. Thus, K=2 gives two well-separated clusters (basically “Gentoo” and “Adelie+Chinstrap”), which yields a higher silhouette score than K=3 where the algorithm is forced to split the Adelie/Chinstrap group and ends up with one weaker separation. In a real analysis, we’d consider this trade-off: do we want 3 clusters to match known species, or 2 clusters that are most distinct in these features? There’s no single correct answer – it depends on the goal. But this illustrates how WCSS and silhouette can guide the choice of K.

## K-Nearest Neighbors (KNN)

Next, let’s switch gears to a **supervised** learning method: K-Nearest Neighbors. KNN is one of the simplest classification algorithms. The basic idea: to predict the class of a new point, look at the “k” closest points in the training data and take a majority vote. In other words, the label is determined by the plurality of the labels of its k nearest neighbors in the training set. If k=5, the 5 closest neighbors’ labels are used – if 3 of them are class A and 2 are class B, the new point is classified as A (majority wins). KNN makes no assumptions about data distribution (it’s a non-parametric, instance-based method), and it’s considered a “lazy” learner because it doesn’t build an explicit model; it just stores the training instances and defers computation to query time.

Choosing k: The parameter k controls the model’s complexity. A small k (e.g. 1) means the classifier can be very flexible and even noisy – essentially memorizing the training data (low bias, high variance). A large k means we smooth over more neighbors, making the classifier more stable but potentially less sensitive to local patterns (high bias, low variance). We’ll see this trade-off in action with a synthetic example.

### Synthetic Dataset with a Nonlinear Boundary

To illustrate KNN, we’ll create a 2D synthetic dataset for binary classification. We want something non-linear to show KNN’s strength at capturing complex boundaries. We generate **100** random points uniformly inside the square
$$
(x_1,x_2)\in[-3,3]
$$

The true boundary is the wiggly curve

$$
x_2 \;=\; \sin(4x_1)+x_1,
$$

so we label a point **class 1** when $x_2$ lies **above** the curve and **class 0** otherwise. This produces two interwoven regions separated by a sine wave, giving KNN a genuinely non‑linear problem.

```{python}
# | echo: false
import numpy as np

# Generate training data -------------------------------------------
np.random.seed(42)  # reproducibility
N = 100  # training points
X1 = np.random.uniform(-3, 3, N)
X2 = np.random.uniform(-3, 3, N)


# Boundary function: sin(4x) + x
def boundary(x):
    return np.sin(4 * x) + x


y = (X2 > boundary(X1)).astype(int)  # 1 if above the curve, else 0

X_train = np.column_stack((X1, X2))
y_train = y
print("Training set class counts:", np.bincount(y_train))

# Generate a test set ------------------------------------------------
np.random.seed(99)
M = 100  # test points
X1_t = np.random.uniform(-3, 3, M)
X2_t = np.random.uniform(-3, 3, M)
y_test = (X2_t > boundary(X1_t)).astype(int)
X_test = np.column_stack((X1_t, X2_t))
print("Test set class counts:", np.bincount(y_test))
```

We have created a training set of 100 points and a test set of 100 points. Let’s visualize the training data and the true boundary:

```{python}
# | echo: false
import matplotlib.pyplot as plt

plt.figure(figsize=(6,5))
plt.scatter(X_train[y_train==0][:,0], X_train[y_train==0][:,1],
            color='blue', label='Class 0')
plt.scatter(X_train[y_train==1][:,0], X_train[y_train==1][:,1],
            color='red', label='Class 1')

# Plot the true decision boundary
x_line = np.linspace(-3, 3, 400)
y_line = boundary(x_line)
plt.plot(x_line, y_line, color='green', linestyle='--', label='True Boundary')

plt.xlabel("$x_1$")
plt.ylabel("$x_2$")
plt.title("Synthetic Training Data and True Boundary ($x_2 = \\sin(4x_1)+x_1$)")
plt.legend()
plt.show()
```

*Synthetic dataset: 100 training points (blue = class 0, red = class 1) in the square 
$[-3,3]^2$

The green dashed line is the true boundary 

$$
x_2 \;=\; \sin(4x_1)+x_1,
$$

You can see that class 0 (blue) occupies the area below the sine curve, and class 1 (red) is above the curve. The boundary wiggles up and down. No linear model could classify this perfectly, but KNN should do well given enough neighbors.

### Implementing KNN from Scratch

The KNN algorithm for classification is straightforward to implement. For each query (test point), we need to find the distances to all training points, pick the k nearest, and take a majority vote of their labels. A basic implementation might be O(n) per query (where n is number of training points), which can be slow for large datasets, but for our data size it’s fine.

We’ll write a function knn_predict(X_train, y_train, X_test, k) that returns predicted labels for the X_test points:

```{python}
# | echo: true
import math


def knn_predict(X_train, y_train, X_test, k):
    predictions = []
    for x in X_test:
        # Compute distance from x to all points in X_train (Euclidean)
        distances = np.linalg.norm(X_train - x, axis=1)
        # Find the indices of the k nearest neighbors
        nn_idx = np.argsort(distances)[:k]
        nn_labels = y_train[nn_idx]
        # Majority vote: the predicted class is the mode of the neighbor labels
        # For binary labels 0/1, we can just take the mean and round it
        vote = 1 if nn_labels.mean() >= 0.5 else 0
        predictions.append(vote)
    return np.array(predictions)


# Try out the KNN predictor for k=5 on a few test points
y_pred_test_k5 = knn_predict(X_train, y_train, X_test[:5], k=5)
print("True labels:     ", y_test[:5])
print("Predicted labels:", y_pred_test_k5)
```

 In practice, one can use spatial data structures (KD-trees, ball trees) for large datasets, but we’re keeping it simple. The majority vote is implemented by averaging the 0/1 labels and rounding – this works because if a majority are 1s, the average > 0.5, otherwise it’s < 0.5 (for multiclass, you’d do something like collections.Counter or np.bincount to get the mode).

Let’s see how our scratch KNN performs and verify it against scikit-learn’s KNN classifier:

```{python}
# | echo: false
# Predict on the entire test set with our function for k=5
y_pred_scratch = knn_predict(X_train, y_train, X_test, k=5)

# Use scikit-learn's KNeighborsClassifier for comparison
from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier(n_neighbors=5)
knn_clf.fit(X_train, y_train)
y_pred_sklearn = knn_clf.predict(X_test)

# Verify that both predictions are the same
print("Our KNN vs sklearn KNN match:", np.array_equal(y_pred_scratch, y_pred_sklearn))
print("Test accuracy (k=5) scratch: %.3f" % (np.mean(y_pred_scratch == y_test)))
print("Test accuracy (k=5) sklearn: %.3f" % (np.mean(y_pred_sklearn == y_test)))
```

If we run this, we find that our implementation’s predictions exactly match scikit-learn’s for k=5 (and indeed they should for any k). The accuracy on the test set for k=5 is around 0.9 (90%). It’s good to confirm that our scratch code is working properly.

### Finding the Best k (Accuracy vs. k)

Finally, let’s see how the choice of k affects the performance on this problem. We’ll evaluate k from 1 up to 30 and record the classification accuracy on the test set for each:

```{python}
# | echo: false
# Evaluate accuracy for k = 1 to 30
accuracies = []
for k in range(1, 31):
    knn_clf = KNeighborsClassifier(n_neighbors=k)
    knn_clf.fit(X_train, y_train)
    acc = knn_clf.score(X_test, y_test)
    accuracies.append(acc)

# Identify the best k
best_k = np.argmax(accuracies) + 1  # +1 because index 0 corresponds to k=1
print("Best k =", best_k, "with test accuracy = %.3f" % max(accuracies))
```

Plotting these accuracies as a function of k:

```{python}
# | echo: false
plt.figure(figsize=(6, 4))
plt.plot(range(1, 31), accuracies, marker="o")
plt.axvline(best_k, color="red", linestyle="--", label=f"Optimal k = {best_k}")
plt.title("KNN Classification Accuracy vs. k")
plt.xlabel("Number of Neighbors (k)")
plt.ylabel("Test Accuracy")
plt.legend()
plt.show()
```

The exact numbers may vary slightly with random seed, but the trend is clear: the highest accuracy in this case is about 92 % for the best k, then it drops off as k increases. The plot of accuracy vs k would show a peak at a very low k, then a gradual decline.

Why does this happen? Our synthetic data has a very clean, deterministic boundary (no noise in labels given the features). A very flexible model like 1-nearest-neighbor can trace that wavy boundary almost perfectly – in fact, 1-NN yields about 92 % accuracy on the test! It slightly overfits to the discrete sample (if we had infinite data, 1-NN would eventually perfectly model the true boundary). Using a larger k smooths the decision boundary. For instance, at k=5 we saw accuracy around 0.9 (90%) – some fine detail is lost, as the model sometimes errs on points near the boundary by averaging in neighbors from the other side of the wave. By k=30, accuracy drops further, because with such a large neighborhood, the classifier is getting too biased – it’s oversmoothing and doesn’t capture the wiggles at all (in the extreme, if you used k equal to the entire training set, it would always predict the majority class, ignoring the input features altogether!). This illustrates the bias-variance trade-off in KNN: small k = low bias, high variance; large k = high bias, low variance. The optimal k is often somewhere in between, and one usually uses cross-validation to find it in practice. In our case, since the data has no label noise, the best performance is at the most flexible end (k=1 or 3).

To visualize what KNN is doing, here’s a depiction of the decision boundary learned by our model for two different k values:
- For k=1, the decision boundary will essentially trace around every red point (each training point defines its own region via a Voronoi partition). It will be very jagged and pass through narrow gaps – perfectly fitting the training data, but potentially too wiggly for noisy data.
- For k=15 (for example), the decision boundary will be much smoother – small irregularities are averaged out, and only broader trends remain. It might misclassify some points that lie in small “bays” of the opposite class, as it favors the overall majority in a larger neighborhood ￼.

In our synthetic example, the boundary was truly sinusoidal, so a very flexible model wins. If we had added noise or outliers, a slightly larger k would likely be better to avoid chasing that noise. This is a great demonstration of how KNN’s parameter k influences the model complexity.

### Conclusion

We’ve created a step-by-step walkthrough of K-Means and K-Nearest Neighbors, implementing both from scratch and validating against library implementations. For K-Means, we clustered penguins by their physical measurements and used WCSS and silhouette analysis to reason about the appropriate number of clusters. For KNN, we built a synthetic classification problem with a non-linear boundary and saw how the choice of neighbors affects performance. Along the way, we visualized the clustering process (with an animated centroid update plot) and the classification boundary (implicitly, through the accuracy vs k behavior and discussion).

Key Takeaways:

- **K-Means** is an unsupervised clustering algorithm that iteratively assigns points to the nearest centroid and updates centroids to the mean of points ￼. It aims to minimize within-cluster variance (WCSS). Choosing the number of clusters K is crucial – methods like the elbow plot and silhouette analysis help evaluate cluster quality.

- **K-Nearest Neighbors** is a simple yet powerful supervised learning method that classifies points based on the majority label of their nearest neighbors. It is easy to implement and makes no distribution assumptions (non-parametric). The parameter k controls the bias-variance trade-off: smaller k can capture fine-grained patterns but may overfit, while larger k smooths out noise but may miss details.

- **Both algorithms are intuitive** K-Means finds natural groupings in data, and KNN makes predictions by “consulting” nearby examples. Despite their simplicity, they are effective for a wide range of problems and are excellent for building intuition about clustering and classification.

Hopefully, this walkthrough helped clarify how K-Means and KNN work. Feel free to experiment with different parameters or datasets – tweak the number of clusters, or try KNN on a different boundary shape – to further solidify your understanding. Happy learning!