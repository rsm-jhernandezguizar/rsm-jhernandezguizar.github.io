---
title: "Patent Success & Airbnb Demand Analysis"
description: "Poisson and negative binomial regression case studies"
image: /images/assignment2.jpg
date: 2025-05-07
author: Juan Hernández Guizar
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---

## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

To start we will review the first 5 data points collected in a table format: 

```{python}
# | echo: false
import pandas as pd

blueprinty = pd.read_csv("Assignment_2_data/blueprinty.csv")
# Preview
blueprinty.head()
```

Below is the distribution of patents among Blueprinty customers vs non-customers. The histogram shows that Blueprinty customers (light orange bars) tend to have more patents on average than non-customers (light blue bars). 

```{python}
# | echo: false
import matplotlib.pyplot as plt
import seaborn as sns

fig, ax = plt.subplots()
sns.histplot(
    data=blueprinty,
    x="patents",
    hue="iscustomer",
    bins=range(0, blueprinty["patents"].max() + 2),
    palette={0: "#ADD8E6", 1: "#FFDAB9"},
    alpha=0.8,
    ax=ax,
)
ax.set_xlabel("Patents in last 5 years")
ax.set_ylabel("Number of firms")
ax.set_title("Patent distribution by Blueprinty customer status")
ax.legend(title="Is Blueprinty customer", labels=["No (0)", "Yes (1)"])
plt.tight_layout()
plt.show()
```

```{python}
# | echo: false
mean_patents = (
    blueprinty.groupby("iscustomer", as_index=False)["patents"]
    .mean()
    .assign(status=lambda d: d["iscustomer"].map({0: "Non-customer", 1: "Customer"}))
)

fig, ax = plt.subplots()
sns.barplot(
    data=mean_patents,
    x="status",
    y="patents",
    hue="status",
    ax=ax,
    palette="Set2",
    dodge=False,  # Ensure bars are not split by hue
)
ax.set_ylabel("Mean patents (5 yrs)")
ax.set_xlabel("")
ax.set_title("Average patents by Blueprinty usage")
plt.tight_layout()
plt.show()
```

On average, Blueprinty customers have approximately 4.13 patents over 5 years, compared to about 3.47 for non-customers. While this naive comparison suggests customers produce more patents, we must consider that Blueprinty’s customers may differ systematically in other ways (e.g. perhaps they are older firms or clustered in certain regions).

Let’s examine the age and regional composition of customers vs non-customers. 

```{python}
# | echo: false
fig, ax = plt.subplots()
sns.histplot(
    data=blueprinty,
    x="age",
    hue="iscustomer",
    kde=True,
    common_norm=False,
    palette={0: "steelblue", 1: "seagreen"},
    alpha=0.6,
    ax=ax,
)
ax.set_xlabel("Firm age (years)")
ax.set_ylabel("Number of firms")
ax.set_title("Age distribution by Blueprinty customer status")
from matplotlib.patches import Patch

handles = [
    Patch(color="steelblue", label="Non-customer"),
    Patch(color="seagreen", label="Customer"),
]
ax.legend(
    handles=handles,
    title="Is customer",
    loc="best",
    frameon=True,
    facecolor="white",
    edgecolor="black",
    fancybox=True,
)
plt.tight_layout()
plt.show()
```

Blueprinty customers have a slightly higher mean age since incorporation (about 26.9 years) than non-customers (26.1 years), but the age distributions largely overlap (both groups are typically 20–30 years old, with only minor differences). This suggests that firm age might not differ dramatically by customer status, though we will account for age in the analysis.

Regionally, there are stark differences in who adopts Blueprinty’s software.

```{python}
# | echo: false
region_counts = (
    blueprinty.groupby(["region", "iscustomer"]).size().reset_index(name="count")
)

fig, ax = plt.subplots()
sns.barplot(
    data=region_counts,
    x="region",
    y="count",
    hue="iscustomer",
    palette={0: "steelblue", 1: "limegreen"},
    ax=ax,
)
ax.set_ylabel("Number of firms")
ax.set_xlabel("Region")
ax.set_title("Firms by region and Blueprinty customer status")
from matplotlib.patches import Patch

handles = [
    Patch(color="steelblue", label="No"),
    Patch(color="limegreen", label="Yes"),
]
ax.legend(
    handles=handles,
    title="Is customer",
    loc="best",
    frameon=True,
    facecolor="white",
    edgecolor="black",
    fancybox=True,
)
plt.tight_layout()
plt.show()
```

Counts of firms by region and Blueprinty customer status. In the Northeast region, the green bar (Blueprinty customers) is higher than the blue bar (non-customers), indicating a large share of Blueprinty’s users are in the Northeast. In contrast, in all other regions (Midwest, South, Southwest, Northwest) the majority of firms are non-customers. This reveals that Blueprinty’s customer base is heavily concentrated in the Northeast, which suggests potential selection bias by region.

Indeed, about 68% of Blueprinty’s customers are located in the Northeast, whereas only ~27% of non-customer firms are in the Northeast. Other regions (Midwest, South, Southwest, Northwest) are under-represented among customers relative to non-customers. This imbalance means any raw difference in patent counts could partly reflect regional effects. In summary, Blueprinty customers tend to have slightly older firms (though age differences are minor) and are much more likely to be in the Northeast region. We will need to control for these factors when analyzing the effect of Blueprinty’s software on patent output.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

Below, $Y_i$ is the patent count for firm $i$ and $\lambda$ is the
average number of patents per firm in five years.

| Symbol | Meaning |
|--------|---------|
| $Y_i$ | Observed patent count for firm $i$ (integer $\ge 0$) |
| $n$ | Total number of firms (length of $\mathbf y$) |
| $\lambda$ | Poisson **rate parameter** — the mean (and variance) of the distribution |
| $\mathbf y$ | Column vector of **all** counts: $\mathbf y = (\,y_{1},\,y_{2},\,\dots,\,y_{n})^{\!\top}$ |
| $\mathcal L(\lambda;\mathbf y)$ | Likelihood of the entire dataset, given $\lambda$ |
| $\ell(\lambda)$ | Log-likelihood, $\ell(\lambda)=\log \mathcal L(\lambda;\mathbf y)$ |

$$
P\!\bigl(Y_i=y_i \mid \lambda\bigr)=
  \frac{e^{-\lambda}\lambda^{y_i}}{y_i!},
  \qquad y_i=0,1,2,\dots
$$

$$
\boxed{
  \mathcal L(\lambda;\mathbf y)=
    e^{-n\lambda}\,
    \lambda^{\sum_{i=1}^{n} y_i}\!
    \Big/
    \prod_{i=1}^{n} y_i!
}
\qquad
\boxed{
  \ell(\lambda)=
    \sum_{i=1}^{n}\!
      \bigl(
        y_i\log\lambda-\lambda-\log y_i!
      \bigr)
}
$$

Putting all counts into one vector lets us write the likelihood compactly and pass the entire dataset to a single log-likelihood function.  In the code below that function is called loglik_poisson.  It follows the “parameter-vector” convention most optimisers expect: the one unknown, lambda, is stored as theta[0].  This style makes the function future-proof—if we later add more parameters we can just extend the theta vector without rewriting the optimiser call.

```{python}
import numpy as np, math


def loglik_poisson(theta, y):
    """
    Poisson log-likelihood

    Parameters
    ----------
    theta : 1-element array-like
        theta[0] = λ  (must be > 0)
    y     : 1-D numpy array of non-negative integers

    Returns
    -------
    float
        Scalar log-likelihood ℓ(λ)
    """
    lam = float(theta[0])  # ← parallels 'mu <- theta[1]'
    if lam <= 0:
        return -np.inf  # guard just like s2>0 in Normal case

    n = y.size
    # ll = Σ(y_i log λ) − n λ − Σ log(y_i!)
    ll = np.sum(y * np.log(lam)) - n * lam - np.sum([math.lgamma(k + 1) for k in y])
    return ll
```    

The curve below shows how the log-likelihood changes as we slide λ across plausible values.  It rises steeply, flattens, and then falls—peaking (unsurprisingly) right where λ equals the sample mean (~3.7 patents).  That single highest point is the Maximum-Likelihood Estimate: the value of λ that makes the observed patent counts most probable under a Poisson model.

```{python}
# | echo: false
import matplotlib.pyplot as plt

y = blueprinty["patents"].values
lam_grid = np.linspace(0.1, 8, 250)
ll_vals = [loglik_poisson([l], y) for l in lam_grid]

fig, ax = plt.subplots()
ax.plot(lam_grid, ll_vals, color="purple")
ax.set_xlabel("λ")
ax.set_ylabel("log-likelihood  ℓ(λ)")
ax.set_title("Poisson log-likelihood – Blueprinty data")
plt.tight_layout()
plt.show()
``` 

$$
P\!\bigl(Y_i = y_i \mid \lambda\bigr)=
  \frac{e^{-\lambda}\lambda^{y_i}}{y_i!},
  \qquad y_i = 0,1,2,\dots
$$

Differentiating the log-likelihood  

$$
\ell(\lambda)=\sum_{i=1}^{n}
  \bigl(
    y_i\log\lambda-\lambda-\log y_i!
  \bigr)
$$

with respect to \(\lambda\) and setting the derivative to zero gives  

$$
\frac{\partial \ell}{\partial \lambda}
  \;=\;
  \frac{\sum_{i=1}^{n}y_i}{\lambda}-n
  \;=\;0
  \;\Longrightarrow\;
  \boxed{\hat\lambda=\bar y}
$$

so the maximum-likelihood estimate is nothing more than the sample mean of the counts. The first code cell reflects that algebra exactly: `y.mean()` is computed and printed as the **Analytic MLE**, which for our data equals 3.6847 patents per firm.  
```{python}
# | echo: false
lambda_mle_formula = y.mean()
print(f"Analytic MLE   λ̂ = {lambda_mle_formula:.4f}")
``` 

```{python}
# | echo: false
from scipy.optimize import minimize_scalar

opt = minimize_scalar(
    lambda l: -loglik_poisson([l], y),   # minimise −ℓ
    bounds=(1e-4, 10), method="bounded"
)
print(f"Optimiser MLE  λ̂ = {opt.x:.4f}")
``` 

The second cell tackles the same task numerically. `scipy.optimize.minimize_scalar` is instructed to minimise the negative log-likelihood (equivalently maximise \(\ell\)), searching over the interval \([10^{-4},10]\). Because the optimiser treats \(\lambda\) as a scalar, we wrap it in a one-element list when passing it to `loglik_poisson`. After a quick line search it returns an **Optimiser MLE** of 3.6847, matching the analytic result to four decimal places—strong confirmation that the calculus and the numerical optimisation tell the same story.

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\lambda_i = e^{X_i'\beta}$. _For example:_

```
poisson_regression_likelihood <- function(beta, Y, X){
   ...
}
```

A covariate (sometimes called a feature or explanatory variable) is simply an observed attribute we believe helps explain the outcome.  Here our covariates are age, age ² (to capture curvature), a set of region dummies, and a binary flag for Blueprinty customer status.  By stacking these in a matrix $X$ and multiplying by a coefficient vector $\boldsymbol\beta$, we let each firm have its own mean rate
$\lambda_i=\exp(X_i^{!\top}\boldsymbol\beta)$—the exponential ensures every $\lambda_i$ stays positive.

| Symbol | Meaning |
|--------|---------|
| $Y_i$ | Observed patent count for firm $i$ (integer $\ge 0$) |
| $X_i$ | Row vector of covariates for firm $i$ (intercept, age, age$^{2}$, region dummies, Blueprinty flag) |
| $\boldsymbol\beta$ | Column vector of coefficients (one per covariate) |
| $\lambda_i$ | Mean patents for firm $i$: $\lambda_i = \exp\!\bigl(X_i^{\!\top}\boldsymbol\beta\bigr)$ |
| $n$ | Total number of firms (rows of $X$) |
| $\mathbf y$ | Column vector of **all** counts: $\mathbf y = (\,y_{1},\,y_{2},\,\dots,\,y_{n})^{\!\top}$ |
| $X$ | Design matrix that stacks all $X_i$ rows |
| $\mathcal L(\boldsymbol\beta;\mathbf y,X)$ | Likelihood of the entire dataset, given $\boldsymbol\beta$ |
| $\ell(\boldsymbol\beta)$ | Log-likelihood, $\ell(\boldsymbol\beta)=\log \mathcal L(\boldsymbol\beta;\mathbf y,X)$ |

Expanding from a constant‐rate model to **Poisson regression** swaps the single parameter $\lambda$ for a whole vector of coefficients $\boldsymbol\beta$.  
Each firm now gets its own mean rate through the inverse-link function  
$\lambda_i=\exp(X_i^{\!\top}\boldsymbol\beta)$, guaranteeing positivity while letting the linear predictor $X_i^{\!\top}\boldsymbol\beta$ wander over the real line.  
The covariate matrix $X$ holds an intercept, age, age², a set of region dummies, and a Blueprinty-customer flag, so any of those characteristics can nudge the expected patent count up or down.

$$
Y_i \,\bigl|\, X_i \;\sim\; \operatorname{Poisson}\!\bigl(\lambda_i\bigr),
\qquad
\lambda_i \;=\; \exp\!\bigl(X_i^{\!\top}\boldsymbol\beta\bigr),
\qquad i = 1,\dots,n.
$$

$$
\mathcal L(\boldsymbol\beta;\mathbf y,X)
  \;=\;
  \prod_{i=1}^{n}
    \frac{e^{-\lambda_i}\,\lambda_i^{\,Y_i}}{Y_i!},
\qquad
\ell(\boldsymbol\beta)
  \;=\;
  \sum_{i=1}^{n}
    \Bigl(
      Y_i\,X_i^{\!\top}\boldsymbol\beta
      \;-\;
      \exp\!\bigl(X_i^{\!\top}\boldsymbol\beta\bigr)
      \;-\;
      \log Y_i!
    \Bigr).
$$

The code block that follows translates this math into Python.  
`loglik_poisson_reg(beta, y, X)` now takes **both** the response vector *and* the covariate matrix, computes the linear predictor $X\boldsymbol\beta$, exponentiates to obtain $\boldsymbol\lambda$, and returns the scalar log-likelihood.  Passing that function to an optimiser (e.g.\ `scipy.optimize.minimize`) yields the maximum-likelihood estimates for the full coefficient vector $\boldsymbol\beta$.

```{python}
# | echo: true
import numpy as np, math


def loglik_poisson_reg(beta, y, X):
    """
    Poisson regression log-likelihood.

    beta : 1-D array, length p           (coefficients)
    y    : 1-D array, length n           (counts)
    X    : 2-D array, shape (n, p)       (covariate matrix)

    Returns
    -------
    float : scalar log-likelihood ℓ(β)
    """
    eta = X @ beta  # linear predictor  η = Xβ  (shape n)
    lam = np.exp(eta)  # inverse link  λ = exp(η)
    if np.any(lam <= 0):
        return -np.inf  # numerical safety
    ll = np.sum(y * eta - lam - [math.lgamma(k + 1) for k in y])
    return ll
``` 

With this function we can now hand the entire β vector to an optimiser (e.g. scipy.optimize.minimize) to obtain maximum-likelihood estimates, just as we did for the single-parameter case—only now the model flexes with age, geography, and Blueprinty adoption.

$$
\mathcal L(\boldsymbol\beta;\mathbf y,X)
  = \prod_{i=1}^{n}
      \frac{e^{-\lambda_i}\lambda_i^{\,Y_i}}{Y_i!},
\qquad
\ell(\boldsymbol\beta)
  = \sum_{i=1}^{n}
      \Bigl(
        Y_i\,X_i^{\!\top}\boldsymbol\beta
        - \exp\!\bigl(X_i^{\!\top}\boldsymbol\beta\bigr)
        - \log Y_i!
      \Bigr).
$$

A **Hessian** is the log-likelihood’s curvature map.  Picture the likelihood
surface as a hill; the Hessian tells us how sharply that hill drops away in
every parameter direction.  Formally

$$
H(\hat{\boldsymbol\beta})
  = -
    \frac{\partial^2\ell(\boldsymbol\beta)}
         {\partial\boldsymbol\beta\,\partial\boldsymbol\beta^{\!\top}}
  \Biggr\rvert_{\;\boldsymbol\beta=\hat{\boldsymbol\beta}},
$$

and its negative inverse is the large-sample covariance of the MLEs, so
\(\operatorname{SE}(\beta_j)=\sqrt{[H^{-1}]_{jj}}\).

```{python}
# | echo: false
import pandas as pd, numpy as np, math
from scipy.optimize import minimize
import statsmodels.api as sm

# ------ design matrix -----------------------------------------------------
df = blueprinty.copy()
df["age2"] = (df["age"] ** 2) / 100  # rescale to avoid overflow
X = pd.get_dummies(df[["age", "age2", "region", "iscustomer"]], drop_first=True)
X.insert(0, "const", 1)
X_mat = X.values.astype(float)
y = df["patents"].values


# ------ log-likelihood ----------------------------------------------------
def loglik_poisson_reg(beta, y, X):
    eta = X @ beta
    lam = np.exp(eta)  # λ_i > 0
    return np.sum(y * eta - lam - [math.lgamma(k + 1) for k in y])


neg_ll = lambda b: -loglik_poisson_reg(b, y, X_mat)

# ------ optimise with BFGS + safe start ----------------------------------
beta0 = np.zeros(X_mat.shape[1])
opt = minimize(neg_ll, beta0, method="BFGS")
beta_hat = opt.x
se_hat = np.sqrt(np.diag(opt.hess_inv))

coef_table = pd.DataFrame(
    {"Coefficient": beta_hat, "Std.Error": se_hat}, index=X.columns
).round(4)
coef_table
``` 

```{python}
# | echo: false
# quick cross-check with statsmodels GLM
model_glm = sm.GLM(y, X_mat, family=sm.families.Poisson())
res_glm = model_glm.fit()
res_glm.summary()
``` 

After rescaling `age²` and bounding the search, the Poisson regression
converges cleanly.  Key take-aways:

* **Age (+) and Age² (−)** form a concave pattern—patenting rises, peaks
  mid-20s, then tapers.
* **Region dummies** shrink toward zero once we explicitly control for
  Blueprinty usage, implying geography itself isn’t the driver; the earlier
  Northeast spike simply reflected the high concentration of customers there.
* **Blueprinty customer (+0.208)** yields ≈ 23 % higher expected patents,
  highly significant even after all other controls.

Hand-rolled MLEs, Hessian-based standard errors, and `statsmodels` GLM all
agree, giving us confidence in the estimates and the narrative.

To translate the log-coefficient into “extra patents,” we ran a simple
counter-factual:

1. **X₀:** keep every firm’s age and region but set `iscustomer = 0`.  
2. **X₁:** identical matrix but flip `iscustomer = 1`.  
3. Predict $\hat y_0=\exp(X_0\hat\beta)$ and $\hat y_1=\exp(X_1\hat\beta)$.  
4. Take the firm-by-firm difference $\hat y_1-\hat y_0$ and average.

```{python}
# | echo: false
import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns

# --- counter-factual predictions (beta_hat, X_mat, X defined earlier) --------
X0, X1 = X_mat.copy(), X_mat.copy()
iscust_idx = X.columns.get_loc("iscustomer")
X0[:, iscust_idx] = 0
X1[:, iscust_idx] = 1

y0 = np.exp(X0 @ beta_hat)
y1 = np.exp(X1 @ beta_hat)
lift = y1 - y0
m0, m1 = y0.mean(), y1.mean()

# --- single figure with two panels -----------------------------------------
fig, ax = plt.subplots(1, 2, figsize=(9, 3))

# density of per-firm lift
sns.kdeplot(lift, fill=True, color="seagreen", ax=ax[0])
ax[0].set_xlabel("Extra patents (Blueprinty − Non-cust.)")
ax[0].set_title("Lift distribution")

# bar chart of group means
ax[1].bar(["Non-cust.", "Blueprinty"], [m0, m1], color=["steelblue", "seagreen"])
ax[1].set_ylabel("Predicted patents / 5 yrs")
ax[1].set_title("Average predicted patents")

plt.tight_layout()
plt.show()  # <-- no savefig, so no extra PNG, no duplicate figure

# summary table
pd.DataFrame({"Non-customer": [m0], "Blueprinty": [m1], "Lift": [m1 - m0]}).round(2)
``` 

The result: **+0.82 patents** per firm over five years, about a **22 % lift**
relative to the baseline mean.  The density plot below shows most firms gain
between 0.5 and 1.1 extra patents, with a long but light right tail for the
largest firms.

### Take-away

After accounting for firm age and regional differences, using Blueprinty still delivers about one additional granted patent every five years. For most engineering shops that’s a solid, tangible boost—enough to nudge a “nice idea” into a fully protected asset on the balance sheet.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::

We treat **number of reviews** as a stand-in for bookings and begin by exploring
the 40,628-listing Airbnb-NYC dataset (features include listing age `days`,
room type, bedrooms, bathrooms, nightly price, review scores for cleanliness /
location / value, and an *instant-bookable* flag).

- **Handling missing values** – 76 listings lack `bedrooms`, 160 lack
  `bathrooms`, and about 10,200 lack all three review-score variables.  
  Most of those 10 k are listings with **zero reviews** (9,481 rows, ≈ 23 % of
  the data).  
  We drop any row with a missing predictor to keep modeling simple, which
  chiefly removes those zero-review listings and leaves **30,160 listings**
  (all with ≥ 1 review).

- **Should we keep the zero-review rows?**  
  Including them would preserve information on brand-new hosts but requires
  imputing their absent review scores or using a two-part model.  
  For this tutorial we exclude them, accepting a bit of bias in exchange for
  cleaner predictors; we flag that trade-off for future work.

- **Feature transformations** –  
  `instant_bookable` is recoded from `'t'/'f'` to **0 / 1**.  
  Nightly `price` is extremely right-skewed, so we model **`log_price`**
  instead, which stabilises variance and gives a roughly bell-shaped
  histogram.  
  `days` remains in raw units (median ≈ 3 years; one outlier appears at
  117 years!), and no further transforms are applied at this stage.

Next, let’s inspect the distribution of our key variables:

```{python}
# | echo: false
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, statsmodels.api as sm, math

raw = pd.read_csv("Assignment_2_data/airbnb.csv", index_col=0)

df = (
    raw.dropna(
        subset=[
            "review_scores_cleanliness",
            "review_scores_location",
            "review_scores_value",
            "bedrooms",
            "bathrooms",
        ]
    )
    .query("number_of_reviews > 0")  # keep ≥1 review
    .assign(
        log_price=lambda d: np.log(d.price),
        instant=lambda d: (d.instant_bookable == "t").astype(int),
    )
)

# print("Rows kept:", df.shape[0])

fig, ax = plt.subplots(1, 2, figsize=(9, 3))
sns.histplot(df.number_of_reviews, bins=60, ax=ax[0])
ax[0].set_title("# reviews (skew!)")
sns.histplot(df.log_price, bins=40, ax=ax[1])
ax[1].set_title("log price (nice)")
plt.tight_layout()
plt.show()
``` 

Left plot: Distribution of the number of reviews per listing (for listings with ≥1 review). The histogram is extremely right-skewed. A large fraction of listings have only a handful of reviews – for example, the median is 8 reviews, and 75% have ≤26 reviews. A long tail of popular listings have many more reviews (the maximum in this subset is 421). This heavy-tailed count distribution suggests that modeling approaches for count data (like Poisson regression) or a log-transformation may be appropriate. Note: ~23% of listings had 0 reviews (not shown here, as they were dropped for modeling), indicating many very new or less-booked listings.

Right plot: Distribution of nightly price, in USD (left), and distribution of log-transformed price (right) for NYC Airbnb listings. The raw price distribution is highly skewed with most listings in the $50–$200 range and a few extreme outliers (up to $10,000). We limited the x-axis to $500 in the left plot for clarity, but even within this range the mass is concentrated at lower prices. The log-scale (natural log) of price, shown on the right, is much more symmetric and bell-shaped. This confirms that a log transformation of price will likely make modeling easier: a unit change in log_price corresponds to a multiplicative change in actual price, and we expect a more linear relationship with outcome variables on that scale.

With the data cleaned and initial insights gathered, we proceed to model the number of reviews (as a proxy for bookings) using two approaches: Poisson regression for count data, and linear regression. The response variable will be the count of reviews. In the linear model, we will use a log transformation of reviews to account for skewness, whereas the Poisson model will use the count directly with a log link function.

### Poisson Model

A log link makes each coefficient a multiplicative bump.

$$
\ell(\beta)=\sum_i\Bigl(y_iX_i’\beta-\exp(X_i’\beta)-\ln(y_i!)\Bigr)
$$

```{python}
# | echo: false
formula = ('number_of_reviews ~ days + bedrooms + bathrooms + log_price + '
           'review_scores_cleanliness + review_scores_location + review_scores_value + '
           'C(room_type) + instant')
pois = sm.GLM.from_formula(formula, data=df,
                           family=sm.families.Poisson()).fit()
print(pois.summary().tables[1])
irr = np.exp(pois.params)
print("\nIncidence-rate ratios (IRR)\n", irr.round(2))
``` 

After fitting the Poisson model we learn, in plain English, that **switching on Instant Book is the single biggest lever: it lifts the expected review count by roughly 42 percent**. A one-point bump in the cleanliness score nudges bookings up by about 11 percent, while each additional year on the platform adds a modest 1 to 2 percent of extra reviews. Bigger homes help at the margin—more bedrooms bring slightly more traffic—whereas adding bathrooms on top of the existing bedroom count appears to signal a pricier, slower-turnover property and nudges counts down. Price itself shows a small positive elasticity once value is controlled, and the classic room-type hierarchy (private > entire place > shared) persists but only at the ten-percent edge. In short, the Poisson coefficients translate into a story where convenience (Instant Book), visible quality (cleanliness), and sensible capacity win the day, while sheer luxury features do not automatically drive higher volume.

### Linear regression on log reviews

We now fit a linear regression model using the same set of predictors, to compare results and illustrate trade-offs. A direct linear model on the count of reviews would violate linearity and normality assumptions (since the outcome is non-negative and very skewed). Therefore, we use $\log(\text{number\_of\_reviews})$ as the response. This means we are modeling the (natural) log of review count, which should yield coefficients that can be interpreted somewhat like elasticities (percent changes). Note that since we dropped zero-review listings, $\log(\text{reviews})$ is defined (for 1 review, log = 0). Had we included zeros, we would need to add a small constant (e.g. log(review+1)) or use Tobit models, but we avoided that issue by excluding zeros earlier.

```{python}
# | echo: false
df["log_reviews"] = np.log(df.number_of_reviews)
ols = sm.OLS.from_formula(formula.replace("number_of_reviews","log_reviews"),
                          data=df).fit()
print(ols.summary().tables[1])
``` 

The linear model summary indicates an $R^2 = 0.036$ (3.6%), meaning the predictors explain only a few percent of the variance in log-reviews. This is extremely low, highlighting that there is a lot of unexplained variability (no surprise given how many idiosyncratic factors affect a listing’s popularity). By contrast, the Poisson’s pseudo-$R^2$ was much higher, but note that pseudo-$R^2$ is not directly comparable to OLS $R^2$ – they measure different things (deviance vs variance explained).

Quick lift chart – turning coefficients into dollars

```{python}
# | echo: false
# Predicted means from the fitted Poisson
pred_base = pois.predict(df.assign(instant=0))  # λ̂ with IB off
pred_IB = pois.predict(df.assign(instant=1))  # λ̂ with IB on
lift = pred_IB - pred_base

sns.kdeplot(lift, fill=True, color="seagreen")
plt.xlabel("Extra reviews (Instant − Base)")
plt.title("Distribution of predicted Instant-Book lift")
plt.tight_layout()
plt.show()

print("Average lift =", lift.mean().round(2), "reviews over the period")
``` 

Most hosts could expect ~6–7 extra reviews (about +40 %) by flipping Instant Book on—substantial given the median listing only has 8.

### Conclusion

Putting everything together, **hosts who activate Instant Book, keep their place immaculately clean, and offer a sensibly-sized listing at a price guests deem fair can expect materially more bookings**—on the order of six to seven extra reviews (≈ 40 %) over the period analysed. Room-type differences are secondary, and charging a premium does not hurt as long as guests still feel the value is there. Because we removed zero-review rows, these insights apply to listings that have at least begun to attract guests; a full funnel analysis would model the “first-review” hurdle separately. Nonetheless, both the Poisson and log-linear models agree on the headline levers, giving us confidence that cleanliness and instant-booking convenience matter far more than whether the sofa faces north or the bath towels are monogrammed.